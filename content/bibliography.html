---
output: md_document
author: "Imke Mayer"
date: 2025-04-04
linktitle: Bibliography
menu: navbar
title: Bibliography
name: Bibliography
identifier: "Bibliography"
url: /bibliography/
weight: 3
---



<p align="justify">
On this platform we attempt to give you an overview of main references on missing values. We do not claim to gather all available references on the subject but rather to offer a peak into different fields of active research on handling missing values, allowing for an introductory reading as well as a starting point for further bibliographical research.
</p>
<p><a href="/bibliography/biblio_complete/"><b>See here for a full (and uncommented) list of references.</b></a></p>
<p align="justify">
Inspired by <a href="https://cran.r-project.org/web/views/MissingData.html" target="_blank"><b>CRAN Task View on Missing Data</b></a> and a <a href="http://journal-sfds.fr/article/view/681" target="_blank">review</a> of Imbert &amp; Vialaneix on handling missing values (2018, written in French) we organized our selection of relevant references on missing values by different topics.
</p>
<div class="container">
<div class="accordion-option">
<a href="javascript:void(0)" class="toggle-accordion active" accordion-id="#accordion"></a>
</div>
<div class="clearfix">

</div>
<div id="accordion" class="panel-group" role="tablist" aria-multiselectable="true">
<!------------------------- INTRODUCTION ------------------------->
<div class="panel panel-default">
<div id="h_notations" class="panel-heading" role="tab">
<h4 class="panel-title">
<a role="button" data-toggle="collapse" data-parent="#accordion" href="#notations" aria-expanded="false" aria-controls="notations">Short introduction to missing values</a>
</h4>
</div>
<div id="notations" class="panel-collapse collapse in" role="tabpanel" aria-labelledby="h_notations">
<div class="panel-body">
<p align="justify">
In order to provide a more formal introduction for the problem of missing values and the existing methods to handle them (e.g. diagnose/describe the missingness or perform statistical analysis on the incomplete data), we introduce some farely standard definitions and notations used in the remainder of this article.
<p>
<ul>
<li><p align="justify">
Let <span class="math inline">\(X=(X_1,\dots, X_p)\)</span> be a vector of <span class="math inline">\(p\)</span> random variables which can be continuous or categorical.
</p></li>
<li><p align="justify">
We note <span class="math inline">\(x_{ij}\)</span> the observation of variable <span class="math inline">\(X_j\)</span> for an individual <span class="math inline">\(i\in\{1,\dots,n\}\)</span> and <span class="math inline">\(\mathbf{x}_i=(x_{i1},\dots,x_{ip})\)</span> the vector of observations of all <span class="math inline">\(p\)</span> variables <span class="math inline">\(X\)</span> for the individual <span class="math inline">\(i\)</span>.
</p></li>
<li><p align="justify">
The observations of the <span class="math inline">\(n\)</span> individuals are stacked by rows in a matrix <span class="math inline">\(\mathbf{X}\in\mathbb{R}^{n\times p}\)</span>.
</p></li>
<li><p align="justify">
The indicator matrix of missing values <span class="math inline">\(\mathbf{R}\)</span> is defined such that its values <span class="math inline">\((r_{ij})_{\substack{i=1,\dots,n\\j=1,\dots,p}}\)</span> are given by: <span class="math inline">\(r_{ij} = \left\{\begin{array}{ll}1 &amp; \text{ if } x_{ij} \text{ is observed}\\0 &amp; \text{ otherwise}\end{array}\right. = \mathbb{1}_{x_{ij}\, is\, observed}\)</span>. The associated random variable is denoted by <span class="math inline">\(R\)</span>.
</p></li>
<li><p align="justify">
The observed and missing parts of <span class="math inline">\(X\)</span> are denoted respectively by <span class="math inline">\(X_{obs}\)</span> and <span class="math inline">\(X_{mis}\)</span>.
</p>
<br>
</div>
</div>
</div></li>
</ul>
<!------------------------- General references ------------------------->
<div class="panel panel-default">
<div id="h_general" class="panel-heading" role="tab">
<h4 class="panel-title">
<a class="collapsed" role="button" data-toggle="collapse" data-parent="#accordion" href="#general" aria-expanded="false" aria-controls="general">General references and reviews</a>
</h4>
</div>
<div id="general" class="panel-collapse collapse in" role="tabpanel" aria-labelledby="h_general">
<div class="panel-body">
<p align="justify">
These general references and reviews are helpful to get started with the large field of missing values as they provide an introduction to the main concepts and methods or give an overview of the diversity of topics in statistical analysis related to missing values. They discuss different mechanisms that generated the missing values, necessary conditions for working consistently on the observed values alone and ways to impute, i.e. complete, the missing values to end up with complete datasets allowing the use of standard statistical analysis methods.
</p>
<!-- Books and book chapters -->
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#gen_books">
Books and book chapters (10)
</button>
<div id="gen_books" class="collapse">
<ul>
<li><pref><cite>Allison, P. D. <em><font color="#428bca">Missing Data</font></em>. Quantitative Applications in the Social Sciences. Thousand Oaks, CA, USA: Sage Publications, 2001. ISBN: 9780761916727.</cite></pref>
<div>
<a href="https://doi.org/10.1136/bmj.38977.682025.2C" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Carpenter, J. and M. Kenward. <em><font color="#428bca">Multiple Imputation and its Application</font></em>. Chichester, West Sussex, UK: Wiley, 2013. ISBN: 9780470740521.</cite></pref>
<div>
<a href="https://doi.org/10.1002/9781119942283" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Enders, C. K. <em><font color="#428bca">Applied Missing Data Analysis</font></em>. Guilford Press, 2010, p. 401. ISBN: 9781606236390.</cite></pref></li>
<li><pref><cite>Kim, J. K. and J. Shao. <em><font color="#428bca">Statistical Methods for Handling Incomplete Data</font></em>. Boca Raton, FL, USA: Chapman and Hall/CRC, 2013. ISBN: 9781482205077.</cite></pref></li>
<li><pref><cite>Little, R. J. and D. B. Rubin. <em><font color="#428bca">Statistical analysis with missing data</font></em>. John Wiley &amp; Sons, 2019.</cite></pref></li>
<li><pref><cite>Molenberghs, G., G. Fitzmaurice, M. G. Kenward, et al. <em><font color="#428bca">Handbook of Missing Data Methodology</font></em>. Chapman &amp; Hall/CRC Handbooks of Modern Statistical Methods. New York, NY, USA: Chapman and Hall/CRC, 2014. ISBN: 9781439854624.</cite></pref></li>
<li><pref><cite>Molenberghs, G. and M. G. Kenward. <em><font color="#428bca">Missing Data in Clinical Studies</font></em>. Chichester, West Sussex, UK: Wiley, 2007. ISBN: 9780470849811.</cite></pref>
<div>
<a href="https://doi.org/10.1002/9780470510445" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>O’Kelly, M. and B. Ratitch. <em><font color="#428bca">Clinical Trials with Missing Data: A Guide for Practitioners</font></em>. John Wiley &amp; Sons, Ltd, 2014.</cite></pref>
<div>
<a href="https://doi.org/10.1002/9781118762516" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Schafer, J. L. <em><font color="#428bca">Analysis of Incomplete Multivariate Data</font></em>. CRC Monographs on Statistics &amp; Applied Probability. Boca Raton, FL, USA: Chapman and Hall/CRC, 1997. ISBN: 0412040611.</cite></pref></li>
<li><pref><cite>Buuren, S. van. <em><font color="#428bca">Flexible Imputation of Missing Data</font></em>. Boca Raton, FL: Chapman and Hall/CRC, 2018.</cite></pref>
<div>
<a href="https://stefvanbuuren.name/fimd/" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
</ul>
</div>
</div>
<!-- Journal articles -->
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#gen_journals">
Journal articles (4)
</button>
<div id="gen_journals" class="collapse">
<ul>
<li><pref><cite>Graham, J. W. <font color="#428bca">Missing data analysis: making it work in the real world</font>. In: <em>Annual Review of Psychology</em> 60 (2009), pp. 549-576.</cite></pref>
<div>
<a href="https://doi.org/10.1146/annurev.psych.58.110405.085530" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Kaiser, J. <font color="#428bca">Dealing with missing values in data</font>. In: <em>Journal of Systems Integration</em> 5.1 (2014), pp. 42-51.</cite></pref>
<div>
<a href="https://doi.org/10.20470/jsi.v5i1.178" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Pigott, T. D. <font color="#428bca">A review of methods for missing data</font>. In: <em>Educational Research and Evaluation</em> 7.4 (2001), pp. 353–383.</cite></pref>
<div>
<a href="https://doi.org/10.1076/edre.7.4.353.8937" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Schafer, J. L. and J. W. Graham. <font color="#428bca">Missing data: our view of the state of the art</font>. In: <em>Psychological Methods</em> 7.2 (2002), pp. 147-177.</cite></pref>
<div>
<a href="https://doi.org/10.1037/1082-989X.7.2.147" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
</ul>
</div>
</div>
<!-- Conference papers -->
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#gen_conf">
Conference papers (1)
</button>
<div id="gen_conf" class="collapse">
<ul>
<li><pref><cite>Orchard, T. and M. A. Woodbury. <font color="#428bca">A missing information principle: theory and applications</font>. In: <em>Proceedings of the Sixth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Theory of Statistic</em>. Ed. by L. M. Le Cam, N. J. and E. L. Scott. Vol. 1. University of California Press, 1972, pp. 697–715.</cite></pref>
<div>
<a href="https://apps.dtic.mil/dtic/tr/fulltext/u2/1022173.pdf" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
</ul>
</div>
</div>
<!-- Reports, theses, etc. -->
<br>
<!-- --------------------------------------------------------------------------------------------------------------------------- -->
<p align="justify">
If you are rather new to the subject and wish to start with less formal and more application-based introductions or if you look for general high-level advices on handling missing data we suggest the following publications:
</p>
<!-- Books and book chapters -->
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#gen2_books">
Books and book chapters (1)
</button>
<div id="gen2_books" class="collapse">
<ul>
<li><pref><cite>National Research Council, U. <em><font color="#428bca">The Prevention and Treatment of Missing Data in Clinical Trials</font></em>. Washington (DC), USA: National Academies Press, 2010. ISBN: 9780309158145.</cite></pref>
<div>
<a href="https://doi.org/10.17226/12955" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
</ul>
</div>
</div>
<!-- Journal articles -->
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#gen2_journals">
Journal articles (6)
</button>
<div id="gen2_journals" class="collapse">
<ul>
<li><pref><cite>Baraldi, A. N. and C. K. Enders. <font color="#428bca">An introduction to modern missing data analysis</font>. In: <em>Journal of School Psychology</em> 48.1 (2010), pp. 5-37.</cite></pref>
<div>
<a href="https://doi.org/10.1016/j.jsp.2009.10.001" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Dax, A. <font color="#428bca">Imputing Missing Entries of a Data Matrix: A review</font>. In: <em>Journal of Advanced Computing</em> 3.3 (2014), pp. 98-222.</cite></pref>
<div>
<a href="https://doi.org/10.7726/jac.2014.1007" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Dong, Y. and C. J. Peng. <font color="#428bca">Principled missing data methods for researchers</font>. In: <em>SpringerPlus</em> 2 (2013), p. 222.</cite></pref>
<div>
<a href="https://doi.org/10.1186/2193-1801-2-222" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Horton, N. J. and K. P. Kleinman. <font color="#428bca">Much Ado About Nothing - A Comparison of Missing Data Methods and Software to Fit Incomplete Data Regression Models</font>. In: <em>The American Statistician</em> 61.1 (2017), pp. 79-90.</cite></pref>
<div>
<a href="https://doi.org/10.1198/000313007X172556" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Meng, X. L. <font color="#428bca">UYou want me to analyze data I don’t have? Are you insane?</font> In: <em>Shanghai Archives of Psychiatry</em> 24.5 (2012), pp. 287-301.</cite></pref>
<div>
<a href="https://doi.org/10.3969/j.issn.1002-0829.2012.05.011" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a><a href="http://stat.harvard.edu/XLM/ShanghaiArchivesofPsychiatry/ShanghaiArchivesofPsychiatry_v24_n5_2012_pp297-301.pdf" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Peugh, J. L. and C. K. Enders. <font color="#428bca">Missing data in educational research: a review of reporting practices and suggestions for improvement</font>. In: <em>Review of Educational Research</em> 74.4 (2004), pp. 525–556.</cite></pref>
<div>
<a href="https://doi.org/10.3102/00346543074004525" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a><a href="http://dx.doi.org/10.3102/00346543074004525" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
</ul>
</div>
</div>
<!-- Conference papers -->
<!-- Reports, theses, etc. -->
<br>
<!-- --------------------------------------------------------------------------------------------------------------------------- -->
<p align="justify">
Furthermore you can have a look at the following statistical journals which regularly contain recent results related to handling missing data:
</p>
<ul>
<li><p><a href="https://rss.onlinelibrary.wiley.com/journal/17409713" target="_blank">Significance</a> (bimonthly magazine)</p></li>
<li><p><a href="http://www3.stat.sinica.edu.tw/statistica/" target="_blank">Statistica sinica</a> (quarterly journal; Volume 28, Number 4, October 2018 on <em>Data Missing Not at Random</em>)</p></li>
<li><p><a href="https://www.imstat.org/journals-and-publications/statistical-science/" target="_blank">Statistical Science</a> (quarterly journal; Volume 33, Number 2, May 2018 on <em>Missing Data</em>)</p></li>
<li><p><a href="https://www.tandfonline.com/action/journalInformation?show=aimsScope&amp;journalCode=utas20" target="_blank">The American Statistician</a> (quarterly journal)
<br></p></li>
</ul>
</div>
</div>
</div>
<!------------------------- Weighting methods ------------------------->
<div class="panel panel-default">
<div id="h_weighting" class="panel-heading" role="tab">
<h4 class="panel-title">
<a class="collapsed" role="button" data-toggle="collapse" data-parent="#accordion" href="#weighting" aria-expanded="false" aria-controls="weighting">Weighting methods</a>
</h4>
</div>
<div id="weighting" class="panel-collapse collapse in" role="tabpanel" aria-labelledby="h_weighting">
<div class="panel-body">
<p align="justify">
The first intuitive and probably most applied solution in data analyses to deal with missing values is to delete the partial observations and to work excusively on the individuals with complete information. This has several drawbacks, among others it introduces an estimation bias in most cases (more precisely in cases where the missingness is not independent of the data). In order to reduce this bias one can reweight the complete observations to compensate for the deletion of incomplete individuals in the dataset. The weights are defined by inverse probabilities, for instance the inverse of the probability for each individual of being fully observed. This method is known as <em>inverse probability weighting</em> and is described in detail in the publications below. We split the references in two parts: handling missing values in survey data and performing causal inference in the presence of missing values, both requiring the use of weighting methods.
</p>
</br>
<!-- --------------------------------------------------------------------------------------------------------------------------- -->
<font size="+1"><b> For survey data analysis</b></font>
<!-- --------------------------------------------------------------------------------------------------------------------------- -->
<p align="justify">
Such weighting methods are widely used on survey data in order to correct for unbalanced sampling fractions by balancing the empirical distributions of the observed covariates to recover the structure of the target population.
</p>
<!-- Books and book chapters -->
<!-- Journal articles -->
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#weight_journals">
Journal articles (9)
</button>
<div id="weight_journals" class="collapse">
<ul>
<li><pref><cite>Buck, S. F. <font color="#428bca">A method of estimation of missing values in multivariate data suitable for use with an electronic computer</font>. In: <em>Journal of the Royal Statistical Society, Series B</em> 22 (1960), pp. 302-306.</cite></pref>
<div>
<a href="https://doi.org/10.1177/004912417700600206" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Carpenter, J. R., M. G. Kenward, and S. Vansteelandt. <font color="#428bca">A comparison of multiple imputation and doubly robust estimation for analyses with missing data</font>. In: <em>Journal of the Royal Statistical Society: Series A (Statistics in Society)</em> 169.3 (2006), pp. 571–584.</cite></pref>
<div>
<a href="https://doi.org/10.1111/j.1467-985X.2006.00407.x" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Fitzmaurice, G. M., G. Molenberghs, and S. R. Lipsitz. <font color="#428bca">Regression Models for Longitudinal Binary Responses with Informative Drop-Outs</font>. In: <em>Journal of the Royal Statistical Society. Series B (Methodological)</em> 57.4 (1995), pp. 691–704.</cite></pref>
<div>
<a href="http://www.jstor.org/stable/2345937" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Gelman, A., G. King, and C. Liu. <font color="#428bca">Not asked and not answered: Multiple imputation for multiple surveys</font>. In: <em>Journal of the American Statistical Association</em> 93.443 (1998), pp. 846–857.</cite></pref>
<div>
<a href="https://doi.org/10.1080/01621459.1998.10473737" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Kalton, G. and D. Kasprzyk. <font color="#428bca">The treatment of missing survey data</font>. In: <em>Survey Methodology</em> 12.1 (1986), pp. 1-16.</cite></pref>
<div>
<a href="http://www.statcan.gc.ca/pub/12-001-x/1986001/article/14404-eng.pdf" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Preisser, J. S., K. K. Lohman, and P. J. Rathouz. <font color="#428bca">Performance of weighted estimating equations for longitudinal binary data with drop-outs missing at random</font>. In: <em>Statistics in Medicine</em> 21.20 (2002), pp. 3035–3054.</cite></pref>
<div>
<a href="https://doi.org/10.1002/sim.1241" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Robins, J. M., A. Rotnitzky, and L. P. Zhao. <font color="#428bca">Estimation of Regression Coefficients When Some Regressors are not Always Observed</font>. In: <em>Journal of the American Statistical Association</em> 89.427 (1994), pp. 846-866.</cite></pref>
<div>
<a href="https://doi.org/10.1080/01621459.1994.10476818" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Rubin, D. B. <font color="#428bca">Formalizing subjective notions about the effect of nonrespondents in sample surveys</font>. In: <em>Journal of the American Statistical Association</em> 72.359 (1977), pp. 538-543.</cite></pref>
<div>
<a href="https://doi.org/10.2307/2286214" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Vansteelandt, S., J. Carpenter, and M. G. Kenward. <font color="#428bca">Analysis of incomplete data using inverse probability weighting and doubly robust estimators</font>. In: <em>Methodology – European Journal of Research Methods for the Behavioral and Social Sciences</em> 6.1 (2010), pp. 37–48.</cite></pref>
<div>
<a href="https://doi.org/10.1027/1614-2241/a000005" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
</ul>
</div>
</div>
<!-- Conference papers -->
<!-- Reports, theses, etc. -->
</br>
<!-- --------------------------------------------------------------------------------------------------------------------------- -->
<font size="+1"><b>Methods in common with causal inference</b></font>
<!-- --------------------------------------------------------------------------------------------------------------------------- -->
<p align="justify">
Inverse probability weighting is also considered in causal inference: A bias is induced by the presence of confounders, i.e. variables which interact with both covariates and outcome. Hence, if the goal is to estimate causal relationships between covariates and outcome it is necessary to account for the potential effect of confounders – a selection bias – on the result of causal inference.
</p>
<!-- Books and book chapters -->
<!-- Journal articles -->
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#weight2_journals">
Journal articles (10)
</button>
<div id="weight2_journals" class="collapse">
<ul>
<li><pref><cite>Bang, H. and J. M. Robins. <font color="#428bca">Doubly robust estimation in missing data and causal inference models</font>. In: <em>Biometrics</em> 61.4 (2005), pp. 962-973.</cite></pref>
<div>
<a href="https://doi.org/10.1111/j.1541-0420.2005.00377.x" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Bartlett, J. W., O. Harel, and J. R. Carpenter. <font color="#428bca">Asymptotically unbiased estimation of exposure odds ratios in complete records logistic regression</font>. In: <em>American journal of epidemiology</em> 182.8 (2015), pp. 730–736.</cite></pref>
<div>
<a href="https://doi.org/10.1093/aje/kwv114" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Blake, H. A., C. Leyrat, K. Mansfield, et al. <font color="#428bca">Propensity scores using missingness pattern information: a practical guide</font>. In: <em>arXiv preprint</em> (2019). arXiv: 1901.03981 [stat.ME].</cite></pref>
<div>
<a href="https://researchonline.lshtm.ac.uk/4651159/1/1901.03981v1.pdf" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Ding, P. and F. Li. <font color="#428bca">Causal Inference: A Missing Data Perspective</font>. In: <em>Statistical Science</em> 33.2 (2018), pp. 214–237.</cite></pref>
<div>
<a href="https://doi.org/10.1214/18-STS645" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Hogan, J. W. and T. Lancaster. <font color="#428bca">Instrumental variables and inverse probability weighting for causal inference from longitudinal observational studies</font>. In: <em>Statistical Methods in Medical Research</em> 13.1 (2004), pp. 17-48.</cite></pref>
<div>
<a href="https://doi.org/10.1191/0962280204sm351ra" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Seaman, S. R. and S. Vansteelandt. <font color="#428bca">Introduction to Double Robust Methods for Incomplete Data</font>. In: <em>Statistical Science</em> 33.2 (2018), p. 184.</cite></pref>
<div>
<a href="https://doi.org/10.1214/18-STS647" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Seaman, S. R. and I. R. White. <font color="#428bca">Review of inverse probability weighting for dealing with missing data</font>. In: <em>Statistical Methods in Medical Research</em> 22.3 (2011), pp. 278-295.</cite></pref>
<div>
<a href="https://doi.org/10.1177/0962280210395740" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Wal, W. M. van der and R. B. Geskus. <font color="#428bca">ipw: an R package for inverse probability weighting</font>. In: <em>Journal of Statistical Software</em> 43.13 (2011).</cite></pref>
<div>
<a href="https://doi.org/10.18637/jss.v043.i13" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Yang, S., L. Wang, and P. Ding. <font color="#428bca">Identification and estimation of causal effects with confounders subject to instrumental missingness</font>. In: <em>Statistics Methodology Repository</em> (2017).</cite></pref>
<div>
<a href="https://arxiv.org/abs/1702.03951" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Zhu, Z., T. Wang, and R. J. Samworth. <font color="#428bca">High-dimensional principal component analysis with heterogeneous missingness</font>. In: <em>arXiv preprint</em> (2019).</cite></pref>
<div>
<a href="https://arxiv.org/abs/1906.12125" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
</ul>
</div>
</div>
<!-- Conference papers -->
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#weight2_conf">
Conference papers (1)
</button>
<div id="weight2_conf" class="collapse">
<ul>
<li><pref><cite>Kallus, N., X. Mao, and M. Udell. <font color="#428bca">Causal Inference with Noisy and Missing Covariates via Matrix Factorization</font>. In: <em>Advances in Neural Information Processing Systems</em>. Ed. by -. 2018. eprint: 1806.00811.</cite></pref>
<div>
<a href="https://arxiv.org/abs/1806.00811" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
</ul>
</div>
</div>
<!-- Reports, theses, etc. -->
<p><br></p>
</div>
</div>
</div>
<!------------------------- Inference ------------------------->
<div class="panel panel-default">
<div id="h_ml" class="panel-heading" role="tab">
<h4 class="panel-title">
<a role="button" data-toggle="collapse" data-parent="#accordion" href="#ml" aria-expanded="false" aria-controls="ml">Inference with missing values</a>
</h4>
</div>
<div id="ml" class="panel-collapse collapse in" role="tabpanel" aria-labelledby="h_ml">
<div class="panel-body">
<p align="justify">
The most popular approach to deal with missing values for statistical inference tasks is likelihood-based approaches that can deal with incomplete data. More precisely, if the missingness mechanism is ignorable (in a certain sense that is explained in the <em>Missing values mechanisms</em> section) then one can attempt to infer the model parameters by maximizing the likelihood on the observed values. When the mechanism cannot be ignored, then a specific model for it needs to be assumed.
The main algorithm available for performing maximum likelihood estimation (ML) with missing values, is the <em>Expectation Maximization</em> (EM) algorithm. This algorithm requires the knowledge of the joint distribution of <span class="math inline">\(X = (X_{obs}, X_{mis})\)</span> and its implementation is not straightforward since it involves integrals which cannot always be computed easily. Once the model parameters are estimated, one can impute the missing values using this estimated information on the data model.
</p>
<p align="justify">
And there exist also other methods that allow for statistical inference with missing values and that are not using likelihood maximization.
</p>
<!-- Books and book chapters -->
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#ml_books">
Books and book chapters (1)
</button>
<div id="ml_books" class="collapse">
<ul>
<li><pref><cite>McLachlan, G. J. and T. Krishnan. <em><font color="#428bca">The EM Algorithm and Extensions</font></em>. Wiley series in probability and statistics. Hoboken, NJ, USA: Wiley, 2008. ISBN: 9780471201700.</cite></pref></li>
</ul>
</div>
</div>
<!-- Journal articles -->
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#ml_journals">
Journal articles (23)
</button>
<div id="ml_journals" class="collapse">
<ul>
<li><pref><cite>Collins, L. M., J. L. Schafer, and K. Chi-Ming. <font color="#428bca">A comparison of inclusive and restrictive strategies in modern missing data procedures</font>. In: <em>Psychological Methods</em> 6.4 (2007), pp. 330-351.</cite></pref>
<div>
<a href="https://doi.org/10.1037/1082-989X.6.4.330" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Enders, C. K. <font color="#428bca">A primer on maximum likelihood algorithms available for use with missing data</font>. In: <em>Structural Equation Modeling</em> 8.1 (2001), pp. 128-141.</cite></pref>
<div>
<a href="https://doi.org/10.1207/S15328007SEM0801_7" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Finkbeiner, C. <font color="#428bca">Estimation for the multiple factor model when data are missing</font>. In: <em>Psychometrika</em> 44.4 (1979), pp. 409-420.</cite></pref>
<div>
<a href="https://doi.org/10.1007/BF02296204" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Golden, R. M., S. S. Henley, H. White, et al. <font color="#428bca">Consequences of model misspecification for maximum likelihood estimation with missing data</font>. In: <em>Econometrics</em> 7.3 (2019), p. 37.</cite></pref>
<div>
<a href="https://doi.org/10.3390/econometrics7030037" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Ibrahim, J. G., M. Chen, and S. R. Lipsitz. <font color="#428bca">Missing responses in generalised linear mixed models when the missing data mechanism is nonignorable</font>. In: <em>Biometrika</em> 88.2 (2001), pp. 551-564.</cite></pref>
<div>
<a href="https://doi.org/10.1093/biomet/88.2.551" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Ibrahim, J. G., S. R. Lipsitz, and M. Chen. <font color="#428bca">Missing Covariates in Generalized Linear Models When the Missing Data Mechanism is Non-Ignorable</font>. In: <em>Journal of the Royal Statistical Society</em>. Series B (Statistical Methodology) 61.1 (1999), pp. 173-190.</cite></pref>
<div>
<a href="https://doi.org/10.1111/1467-9868.00170" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a><a href="http://www.jstor.org/stable/2680744" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Jiang, W., J. Josse, and M. Lavielle. <font color="#428bca">Logistic Regression with Missing Covariates–Parameter Estimation, Model Selection and Prediction</font>. In: <em>arXiv preprint</em> (2018). arXiv: 1805.04602 [stat.ME].</cite></pref></li>
<li><pref><cite>Jones, M. P. <font color="#428bca">Indicator and Stratification Methods for Missing Explanatory Variables in Multiple Linear Regression</font>. In: <em>Journal of the American Statistical Association</em> 91.433 (1996), pp. 222-230.</cite></pref>
<div>
<a href="https://doi.org/10.1080/01621459.1996.10476680" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Little, R. J. A. <font color="#428bca">Regression with missing X’s: a review</font>. In: <em>Journal of the American Statistical Association</em> 87.420 (1992), pp. 1227-1237.</cite></pref>
<div>
<a href="https://doi.org/10.2307/2290664" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Louis, T. A. <font color="#428bca">Finding the Observed Information Matrix when Using the EM Algorithm</font>. In: <em>Journal of the Royal Statistical Society. Series B (Methodological)</em> 44.2 (1982), pp. 226–233.</cite></pref>
<div>
<a href="http://www.jstor.org/stable/2345828" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Lüdtke, O., A. Robitzsch, and S. G. West. <font color="#428bca">Regression models involving nonlinear effects with missing data: A sequential modeling approach using Bayesian estimation.</font> In: <em>Psychological methods</em> (2019).</cite></pref>
<div>
<a href="https://doi.org/10.1037/met0000233" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Meng, S. L. and D. B. Rubin. <font color="#428bca">Maximum likelihood estimation via the ECM algorithm: a general framework</font>. In: <em>Biometrika</em> 80.2 (1993), pp. 267-278.</cite></pref>
<div>
<a href="https://doi.org/10.1093/biomet/80.2.267" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Meng, X. L. and D. B. Rubin. <font color="#428bca">Using EM to obtain asymptotic variance-covariance matrices: the SEM algorithm</font>. In: <em>Journal of the American Statistical Association</em> 86.416 (1991), pp. 899-909.</cite></pref>
<div>
<a href="https://doi.org/10.1080/01621459.1991.10475130" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Rosseel, Y. <font color="#428bca">lavaan: an R package for structural equation modeling</font>. In: <em>Journal of Statistical Software</em> 48.2 (2012).</cite></pref>
<div>
<a href="https://doi.org/10.18637/jss.v048.i02" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Rubin, D. B. <font color="#428bca">Inference and missing data</font>. In: <em>Biometrika</em> 63.3 (1976), pp. 581-592.</cite></pref>
<div>
<a href="https://doi.org/10.1093/biomet/63.3.581" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Stubbendick, A. L. and J. G. Ibrahim. <font color="#428bca">Maximum Likelihood Methods for Nonignorable Missing Responses and Covariates in Random Effects Models</font>. In: <em>Biometrics</em> 59.4 (2003), pp. 1140–1150.</cite></pref>
<div>
<a href="https://doi.org/10.1111/j.0006-341X.2003.00131.x" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Stubbendick, A. L. and J. G. Ibrahim. <font color="#428bca">Likelihood-based inference with nonignorable missing responses and covariates in models for discrete longitudinal data</font>. In: <em>Statistica Sinica</em> 16.4 (2006), pp. 1143–1167.</cite></pref>
<div>
<a href="https://www.jstor.org/stable/24307781" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Tabouy, T., P. Barbillon, and J. Chiquet. <font color="#428bca">Variational inference for stochastic block models from sampled data</font>. In: <em>Journal of the American Statistical Association</em> 115.529 (2020), pp. 455–466.</cite></pref>
<div>
<a href="https://doi.org/https://doi.org/10.1080/01621459.2018.1562934" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Tchetgen Tchetgen, E. J., L. Wang, and B. Sun. <font color="#428bca">Discrete choice models for nonmonotone nonignorable missing data: identification and inference</font>. In: <em>Statistica Sinica</em> 28.4 (2018), pp. 2069–2088.</cite></pref>
<div>
<a href="https://doi.org/10.5705/ss.202016.0325" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Xue, F. and A. Qu. <font color="#428bca">Integrating multi-source block-wise missing data in model selection</font>. In: <em>Journal of the American Statistical Association</em> (2020), pp. 1–36.</cite></pref>
<div>
<a href="https://doi.org/10.1080/01621459.2020.1751176" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Zhao, Y. <font color="#428bca">Statistical inference for missing data mechanisms</font>. In: <em>Statistics in Medicine</em> 39.28 (2020), pp. 4325–4333.</cite></pref>
<div>
<a href="https://doi.org/10.1002/sim.8727" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Zhao, J. and Y. Ma. <font color="#428bca">A versatile estimation procedure without estimating the nonignorable missingness mechanism</font>. In: <em>Journal of the American Statistical Association</em> (2021), pp. 1–15.</cite></pref>
<div>
<a href="https://doi.org/10.1080/01621459.2021.1893176" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Zhou, Y., R. J. A. Little, and J. D. Kalbfleisch. <font color="#428bca">Block-conditional missing at random models for missing data</font>. In: <em>Statistical Science</em> 25.4 (2010), pp. 517–532.</cite></pref>
<div>
<a href="https://doi.org/10.1214/10-STS344" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
</ul>
</div>
</div>
<!-- Conference papers -->
<!-- Reports, theses, etc. -->
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#ml_misc">
Reports, theses, etc. (1)
</button>
<div id="ml_misc" class="collapse">
<ul>
<li><pref><cite>Londschien, M., S. Kovács, and P. Bühlmann. <em><font color="#428bca">Change point detection for graphical models in presence of missing values</font></em>. 2019. arXiv: 1907.05409 [stat.ML].</cite></pref></li>
</ul>
</div>
</div>
<p><br></p>
<p><font size="+1"><b>Regression</b></font></p>
<p align="justify">
There is a vast literature on how to perform (linear) regression, possibly in high dimensional setting, in presence of missing values in the covariates. This can be seen as a particular case of supervised learning, which is presented below even if the focus is often more on estimating parameters or selecting relevant variables.
</p>
<!-- Books and book chapters -->
<!-- Journal articles -->
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#regr_journals">
Journal articles (5)
</button>
<div id="regr_journals" class="collapse">
<ul>
<li><pref><cite>Jiang, W., M. Bogdan, J. Josse, et al. <font color="#428bca">Adaptive Bayesian SLOPE–High-dimensional Model Selection with Missing Values</font>. In: <em>arXiv preprint</em> (2019).</cite></pref>
<div>
<a href="https://arxiv.org/abs/1907.10867" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Golden, R. M., S. S. Henley, H. White, et al. <font color="#428bca">Consequences of model misspecification for maximum likelihood estimation with missing data</font>. In: <em>Econometrics</em> 7.3 (2019), p. 37.</cite></pref>
<div>
<a href="https://doi.org/10.3390/econometrics7030037" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Jiang, W., J. Josse, and M. Lavielle. <font color="#428bca">Logistic Regression with Missing Covariates–Parameter Estimation, Model Selection and Prediction</font>. In: <em>arXiv preprint</em> (2018). arXiv: 1805.04602 [stat.ME].</cite></pref></li>
<li><pref><cite>Jones, M. P. <font color="#428bca">Indicator and Stratification Methods for Missing Explanatory Variables in Multiple Linear Regression</font>. In: <em>Journal of the American Statistical Association</em> 91.433 (1996), pp. 222-230.</cite></pref>
<div>
<a href="https://doi.org/10.1080/01621459.1996.10476680" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Lüdtke, O., A. Robitzsch, and S. G. West. <font color="#428bca">Regression models involving nonlinear effects with missing data: A sequential modeling approach using Bayesian estimation.</font> In: <em>Psychological methods</em> (2019).</cite></pref>
<div>
<a href="https://doi.org/10.1037/met0000233" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
</ul>
</div>
</div>
<!-- Conference papers -->
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#regr_conf">
Conference papers (1)
</button>
<div id="regr_conf" class="collapse">
<ul>
<li><pref><cite>Loh, P. and M. J. Wainwright. <font color="#428bca">High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity</font>. In: <em>Advances in Neural Information Processing Systems</em>. Ed. by J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira and K. Q. Weinberger. Vol. 24. Curran Associates, Inc., 2011, pp. 2726–2734.</cite></pref>
<div>
<a href="https://proceedings.neurips.cc/paper/2011/file/ab541d874c7bc19ab77642849e02b89f-Paper.pdf" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
</ul>
</div>
</div>
<!-- Reports, theses, etc. -->
<p><br></p>
</div>
</div>
</div>
<!------------------------- Single Imputation ------------------------->
<div class="panel panel-default">
<div id="h_imp" class="panel-heading" role="tab">
<h4 class="panel-title">
<a role="button" data-toggle="collapse" data-parent="#accordion" href="#imp" aria-expanded="false" aria-controls="imp">Single imputation</a>
</h4>
</div>
<div id="imp" class="panel-collapse collapse in" role="tabpanel" aria-labelledby="h_imp">
<div class="panel-body">
<p align="justify">
In the previously mentioned EM algorithm there is in fact an implicit step called <em>imputation</em>: imputing a missing value means replacing it with a <em>plausible</em> one. The definition of <em>plausibility</em> is not stated explicitly but can be deduced from the used method to <em>fill in</em> the gaps, for instance one could choose to replace all missing values of a certain variable <span class="math inline">\(X_j\)</span> by the average observed value <span class="math inline">\(\frac{1}{n_{obs,j}}\sum_{i} x_{ij}\mathbb{1}_{\{x_{ij} \, is\, observed\}}\)</span>, where <span class="math inline">\(n_{obs,j} = \sum_{i} \mathbb{1}_{\{x_{ij} \, is\, observed\}}\)</span>. The interest of imputation is manifold: (1) it allows to use all information in the sample (instead of deleting incomplete observations which leads to a decreasing power in the statistical analysis), (2) if there is sufficient data, i.e. sufficient observations, then the imputation can be very accurate and this assures good quality of future statistical analyses and (3) the imputed dataset is a complete dataset and one can apply standard statistical inference methods. The latter however has to be treated with caution since it implies that in the statistical analysis one does not make any distinction between observed values and imputed values anymore. We will come back to this issue in the next section on <em>multiple imputation</em>.
</p>
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#imp_journals">
Journal articles (27)
</button>
<div id="imp_journals" class="collapse">
<ul>
<li><pref><cite>Audigier, V., F. Husson, and J. Josse. <font color="#428bca">A principal component method to impute missing values for mixed data</font>. In: <em>Advances in Data Analysis and Classification</em> 10.1 (2016), pp. 5-26.</cite></pref>
<div>
<a href="https://doi.org/10.1007/s11634-014-0195-1" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Bertsimas, D., C. Pawlowski, and Y. D. Zhuo. <font color="#428bca">From predictive methods to missing data imputation: an optimization approach</font>. In: <em>The Journal of Machine Learning Research</em> 18.1 (2017), pp. 7133–7171.</cite></pref></li>
<li><pref><cite>Cranmer, S. J. and J. Gill. <font color="#428bca">We have to be discrete about this: a non-parametric imputation technique for missing categorical data</font>. In: <em>British Journal of Political Science</em> 43 (2012), pp. 425-449.</cite></pref>
<div>
<a href="https://doi.org/10.1017/S0007123412000312" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Crookston, N. L. and A. O. Finley. <font color="#428bca">yaImpute: an R package for kNN imputation</font>. In: <em>Journal of Statistical Software</em> 23 (2008), p. 10.</cite></pref>
<div>
<a href="https://doi.org/10.18637/jss.v023.i10" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Dax, A. <font color="#428bca">Imputing Missing Entries of a Data Matrix: A review</font>. In: <em>Journal of Advanced Computing</em> 3.3 (2014), pp. 98-222.</cite></pref>
<div>
<a href="https://doi.org/10.7726/jac.2014.1007" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Ding, Y. and J. S. Simonoff. <font color="#428bca">An investigation of missing data methods for classification trees applied to binary response data</font>. In: <em>Journal of Machine Learning Research</em> 11.1 (2010), pp. 131-170.</cite></pref>
<div>
<a href="http://www.jmlr.org/papers/v11/ding10a.html" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Fellegi, I. P. and D. Holt. <font color="#428bca">A systematic approach to automatic edit and imputation</font>. In: <em>Journal of the American Statistical Association</em> 71.353 (1976), pp. 17-35.</cite></pref>
<div>
<a href="https://doi.org/10.2307/2285726" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Ferrari, P. A., P. Annoni, A. Barbiero, et al. <font color="#428bca">An imputation method for categorical variables with application to nonlinear principal component analysis</font>. In: <em>Computational Statistics &amp; Data Analysis</em> 55.7 (2011), pp. 2410-2420.</cite></pref>
<div>
<a href="https://doi.org/10.1016/j.csda.2011.02.007" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Finkbeiner, C. <font color="#428bca">Estimation for the multiple factor model when data are missing</font>. In: <em>Psychometrika</em> 44.4 (1979), pp. 409-420.</cite></pref>
<div>
<a href="https://doi.org/10.1007/BF02296204" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Huisman, M. <font color="#428bca">Imputation of missing item responses: some simple techniques</font>. In: <em>Quality &amp; Quantity</em> 34.4 (2000), pp. 331-351.</cite></pref>
<div>
<a href="https://doi.org/10.1023/A:1004782230065" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Husson, F. and J. Josse. <font color="#428bca">Handling missing values in multiple factor analysis</font>. In: <em>Food Quality and Preference</em> 30 (2013), pp. 77-85.</cite></pref>
<div>
<a href="https://doi.org/10.1016/j.foodqual.2013.04.013" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Ilin, A. and T. Raiko. <font color="#428bca">Practical approaches to Principal Component Analysis in the presence of missing values</font>. In: <em>Journal of Machine Learning Research</em> 11 (2010), pp. 1957-2000.</cite></pref>
<div>
<a href="http://jmlr.csail.mit.edu/papers/v11/ilin10a.html" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Joenssen, D. W. and U. Bankhofer. <font color="#428bca">Donor limited hot deck imputation: effect on parameter estimation</font>. In: <em>Journal of Theoretical and Applied Computer Science</em> 6.3 (2012), pp. 58-70.</cite></pref>
<div>
<a href="http://www.jtacs.org/archive/2012/3/6" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Josse, J., M. Chavent, B. Liquet, et al. <font color="#428bca">Handling missing values with regularized iterative multiple correspondance analysis</font>. In: <em>Journal of Classification</em> 29.1 (2012), pp. 91-116.</cite></pref>
<div>
<a href="https://doi.org/10.1007/s00357-012-9097-0" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Josse, J., F. Husson, and J. Pagès. <font color="#428bca">Gestion des données manquantes en Analyse en Composantes Principales</font>. In: <em>Journal de la Société Française de Statistique</em> 150.2 (2009), pp. 28-51.</cite></pref>
<div>
<a href="http://journal-sfds.fr/ojs/index.php/J-SFdS/article/view/33/27" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Kalton, G. and D. Kasprzyk. <font color="#428bca">The treatment of missing survey data</font>. In: <em>Survey Methodology</em> 12.1 (1986), pp. 1-16.</cite></pref>
<div>
<a href="http://www.statcan.gc.ca/pub/12-001-x/1986001/article/14404-eng.pdf" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Kohn, R. and C. F. Ansley. <font color="#428bca">Estimation, prediction, and interpolation for ARIMA models with missing data</font>. In: <em>Journal of the American Statistical Association</em> 81.395 (1986), pp. 751-761.</cite></pref>
<div>
<a href="https://doi.org/10.2307/2289007" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Kowarik, A. and M. Templ. <font color="#428bca">Imputation with the R Package VIM</font>. In: <em>Journal of Statistical Software</em> 74.7 (2016), pp. 1-16.</cite></pref>
<div>
<a href="https://doi.org/10.18637/jss.v074.i07" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Moritz, S. and T. Bartz-Beielstein. <font color="#428bca">imputeTS: time series missing value imputation in R</font>. In: <em>The R Journal</em> 9.1 (2017), pp. 207-218.</cite></pref>
<div>
<a href="https://journal.r-project.org/archive/2017/RJ-2017-009/index.html" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Tang, F. and H. Ishwaran. <font color="#428bca">Random forest missing data algorithms</font>. In: <em>Statistical Analysis and Data Mining: The ASA Data Science Journal</em> 10.6 (2017), pp. 363–377.</cite></pref>
<div>
<a href="https://doi.org/https://doi.org/10.1002/sam.11348" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Stacklies, W., H. Redestig, M. Scholz, et al. <font color="#428bca">pcaMethods – a bioconductor package providing PCA methods for incomplete data</font>. In: <em>Bioconductor</em> 23.9 (2007), pp. 1164-1167.</cite></pref>
<div>
<a href="https://doi.org/10.1093/bioinformatics/btm069" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Troyanskaya, O., M. Cantor, G. Sherlock, et al. <font color="#428bca">Missing value estimation methods for DNA microarrays</font>. In: <em>Bioinformatics</em> 17.6 (2001), pp. 520-525.</cite></pref>
<div>
<a href="https://doi.org/10.1093/bioinformatics/17.6.520" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Unnebrink, K. and J. Windeler. <font color="#428bca">Intention-to-treat: methods for dealing with missing values in clinical trials of progressively deteriorating diseases</font>. In: <em>Statistics in Medecine</em> 20.24 (2001), pp. 3931-3946.</cite></pref>
<div>
<a href="https://doi.org/10.1002/sim.1149" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Verbanck, M., J. Josse, and F. Husson. <font color="#428bca">Regularised PCA to denoise and visualise data</font>. In: <em>Statistics and Computing</em> 25.2 (2015), pp. 471-486.</cite></pref>
<div>
<a href="https://doi.org/10.1007/s11222-013-9444-y" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Zhang, H., P. Xie, and E. Xing. <font color="#428bca">Missing Value Imputation Based on Deep Generative Models</font>. In: <em>Computing Research Repository</em> abs/1808.01684 (2018).</cite></pref>
<div>
<a href="https://arxiv.org/abs/1808.01684" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Zhang, S. <font color="#428bca">Nearest neighbor selection for iterative kNN imputation</font>. In: <em>Journal of Systems and Software</em> 85.11 (2012), pp. 2541-2552.</cite></pref>
<div>
<a href="https://doi.org/10.1016/j.jss.2012.05.073" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Zhu, Z., T. Wang, and R. J. Samworth. <font color="#428bca">High-dimensional principal component analysis with heterogeneous missingness</font>. In: <em>arXiv preprint</em> (2019).</cite></pref>
<div>
<a href="https://arxiv.org/abs/1906.12125" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
</ul>
</div>
</div>
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#imp_conf">
Conference papers (2)
</button>
<div id="imp_conf" class="collapse">
<ul>
<li><pref><cite>Tran, L., X. Liu, J. Zhou, et al. <font color="#428bca">Missing Modalities Imputation via Cascaded Residual Autoencoder</font>. In: <em>2017 IEEE Conference on Computer Vision and PAttern Recognition (CVPR)</em>. (Jul. 21, 2017-Jul. 26, 2017). Ed. by -. IEEE, 2017, pp. 4971-4980.</cite></pref>
<div>
<a href="https://doi.org/10.1109/CVPR.2017.528" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Zhao, Y. and M. Udell. <font color="#428bca">Missing value imputation for mixed data via gaussian copula</font>. In: <em>Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em>. 2020, pp. 636–646.</cite></pref>
<div>
<a href="https://doi.org/10.1145/3394486.3403106" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
</ul>
</div>
</div>
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#imp_misc">
Reports, theses, etc. (1)
</button>
<div id="imp_misc" class="collapse">
<ul>
<li><pref><cite>Moritz, S., A. Sardá, T. Bartz-Beielstein, et al. <font color="#428bca">Comparison of different methods for univariate time series imputation in R</font>. Prepint arXiv 1510.03924. 2015.</cite></pref>
<div>
<a href="https://arxiv.org/abs/1510.03924" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
</ul>
</div>
</div>
<p><br></p>
<!-- Hot deck and KNN-->
<p><font size="+1"><b>Hot-deck and KNN approaches</b></font></p>
<p align="justify">
Let <span class="math inline">\(x_i\)</span> be an observation with missing values, e.g. each entry of <span class="math inline">\(x_i\)</span> could be the temperature at a certain day for one given place and unfortunately for some days the temperature was not measured. An intuitive idea to replace this missing information could be: take other observations <span class="math inline">\(\{x_j\}_j\)</span> which are similar to <span class="math inline">\(x_i\)</span> at the observed values and use this information to <em>fill in</em> the gaps. This idea of taking observed values from <em>neighbours</em> or <em>donors</em> based on some similarity measure is implemented in the so-called <em>hot-deck</em> and <em>k-nearest-neighbors</em> (kNN) approaches.
</p>
<!-- Books and book chapters -->
<!-- Journal articles -->
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#hot_journals">
Journal articles (7)
</button>
<div id="hot_journals" class="collapse">
<ul>
<li><pref><cite>Andridge, R. and R. J. A. Little. <font color="#428bca">A review of hot deck imputation for survey non-response</font>. In: <em>International Statistical Review</em> 78.1 (2010), pp. 40-64.</cite></pref>
<div>
<a href="https://doi.org/10.1111/j.1751-5823.2010.00103.x" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Huisman, M. <font color="#428bca">Imputation of missing item responses: some simple techniques</font>. In: <em>Quality &amp; Quantity</em> 34.4 (2000), pp. 331-351.</cite></pref>
<div>
<a href="https://doi.org/10.1023/A:1004782230065" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Imbert, A., A. Valsesia, C. Le Gall, et al. <font color="#428bca">Multiple hot-deck imputation for network inference from RNA sequencing data</font>. In: <em>Bioinformatics</em> 34.10 (2018), pp. 1726-1732.</cite></pref>
<div>
<a href="https://doi.org/10.1093/bioinformatics/btx819" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Joenssen, D. W. and U. Bankhofer. <font color="#428bca">Donor limited hot deck imputation: effect on parameter estimation</font>. In: <em>Journal of Theoretical and Applied Computer Science</em> 6.3 (2012), pp. 58-70.</cite></pref>
<div>
<a href="http://www.jtacs.org/archive/2012/3/6" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Rao, J. N. K. and J. Shao. <font color="#428bca">Jackknife variance estimation with survey data under hot deck imputation</font>. In: <em>Biometrika</em> 79.4 (1992), pp. 811-822.</cite></pref>
<div>
<a href="https://doi.org/10.2307/2337236" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Reilly, M. and M. Pepe. <font color="#428bca">The relationship between hot-deck multiple imputation and weighted likelihood</font>. In: <em>Statistics in Medecine</em> 16.1-3 (1997), pp. 5-19.</cite></pref>
<div>
<a href="https://doi.org/10.1002/(SICI)1097-0258(19970115)16:1%3C5::AID-SIM469%3E3.0.CO;2-8" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Voillet, V., P. Besse, L. Liaubet, et al. <font color="#428bca">Handling missing rows in multi-omics data integration: multiple imputation in multiple factor analysis framework</font>. In: <em>BMC Bioinformatics</em> 17.402 (2016). Forthcoming.</cite></pref>
<div>
<a href="https://doi.org/10.1186/s12859-016-1273-5" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
</ul>
</div>
</div>
<!-- Conference papers -->
<!-- Reports, theses, etc. -->
<p><br></p>
<!-- Matrix factorization -->
<font size="+1"><b>Matrix factorization</b></font>
<p align="justify">
A special case of imputation is matrix completion that exploits structural assumptions about the row and column spaces to impute the missing values.
</p>
<!-- Books and book chapters -->
<!-- Journal articles -->
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#mf_journals">
Journal articles (3)
</button>
<div id="mf_journals" class="collapse">
<ul>
<li><pref><cite>Nguyen, L. T., J. Kim, and B. Shim. <font color="#428bca">Low-Rank Matrix Completion: A Contemporary Survey</font>. In: <em>IEEE Access</em> 7 (2019), pp. 94215–94237.</cite></pref>
<div>
<a href="https://doi.org/10.1109/ACCESS.2019.2928130" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Robin, G., O. Klopp, J. Josse, et al. <font color="#428bca">Main Effects and Interactions in Mixed and Incomplete Data Frames</font>. In: <em>Journal of the American Statistical Association</em> 115.531 (2020), pp. 1292-1303. eprint: <a href="https://doi.org/10.1080/01621459.2019.1623041" class="uri">https://doi.org/10.1080/01621459.2019.1623041</a>.</cite></pref>
<div>
<a href="https://doi.org/10.1080/01621459.2019.1623041" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a><a href=" https://doi.org/10.1080/01621459.2019.1623041" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Sportisse, A., C. Boyer, and J. Josse. <font color="#428bca">Imputation and low-rank estimation with Missing Not At Random data</font>. In: <em>Statistics and Computing</em> 30.6 (2018), pp. 1629-1643.</cite></pref>
<div>
<a href="https://doi.org/10.1007/s11222-020-09963-5" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
</ul>
</div>
</div>
<!-- Conference papers -->
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#mf_conf">
Conference papers (3)
</button>
<div id="mf_conf" class="collapse">
<ul>
<li><pref><cite>Kallus, N., X. Mao, and M. Udell. <font color="#428bca">Causal Inference with Noisy and Missing Covariates via Matrix Factorization</font>. In: <em>Advances in Neural Information Processing Systems</em>. Ed. by -. 2018. eprint: 1806.00811.</cite></pref>
<div>
<a href="https://arxiv.org/abs/1806.00811" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Ma, W. and G. H. Chen. <font color="#428bca">Missing Not at Random in Matrix Completion: The Effectiveness of Estimating Missingness Probabilities Under a Low Nuclear Norm Assumption</font>. In: <em>Advances in Neural Information Processing Systems 32</em>. Ed. by H. Wallach, H. Larochelle, A. Beygelzimer, F. d. Alché-Buc, E. Fox and R. Garnett. Curran Associates, Inc., 2019, pp. 14900–14909.</cite></pref>
<div>
<a href="http://papers.nips.cc/paper/9628-missing-not-at-random-in-matrix-completion-the-effectiveness-of-estimating-missingness-probabilities-under-a-low-nuclear-norm-assumption.pdf" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Zhao, Y. and M. Udell. <font color="#428bca">Missing value imputation for mixed data via gaussian copula</font>. In: <em>Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em>. 2020, pp. 636–646.</cite></pref>
<div>
<a href="https://doi.org/10.1145/3394486.3403106" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
</ul>
</div>
</div>
<!-- Reports, theses, etc. -->
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#mf_misc">
Reports, theses, etc. (1)
</button>
<div id="mf_misc" class="collapse">
<ul>
<li><pref><cite>Robin, G. <em><font color="#428bca">Low-rank methods for heterogeneous and multi-source data</font></em>. 2019.</cite></pref>
<div>
<a href="https://doi.org/https://www.theses.fr/2019SACLX026" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
</ul>
</div>
</div>
<p><br></p>
</div>
</div>
</div>
<!------------------------- Multiple imputation ------------------------->
<div class="panel panel-default">
<div id="h_mi" class="panel-heading" role="tab">
<h4 class="panel-title">
<a role="button" data-toggle="collapse" data-parent="#accordion" href="#mi" aria-expanded="false" aria-controls="mi">Multiple imputation</a>
</h4>
</div>
<div id="mi" class="panel-collapse collapse in" role="tabpanel" aria-labelledby="h_mi">
<div class="panel-body">
<p align="justify">
A major drawback of single imputation, i.e. where every missing value is replaced by a single most plausible value, consists in the underestimation of the overall variance of the data and inferred parameters. Indeed, by replacing every missing value by a given <em>plausible</em> one and by applying generic statistical methods on the completed dataset, one makes no difference between initially observed and unobserved data anymore. Therefore the variability due to the uncertainty of the missing values is not reflected in future statistical analyses which treat the dataset as if it had been fully observed from the beginning. A nice and conceptually simple workaround for this problem is <em>multiple imputation</em>: instead of generating a single complete dataset by a given imputation method one imputes every missing value by several possible values. Statistical analysis is then applied on each of the imputed datasets and the resulting estimations are aggregated and used to estimate the sample variance and the variance due to the uncertainty in the missing values.
</p>
<!-- Books and book chapters -->
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#mi_books">
Books and book chapters (2)
</button>
<div id="mi_books" class="collapse">
<ul>
<li><pref><cite>Carpenter, J. and M. Kenward. <em><font color="#428bca">Multiple Imputation and its Application</font></em>. Chichester, West Sussex, UK: Wiley, 2013. ISBN: 9780470740521.</cite></pref>
<div>
<a href="https://doi.org/10.1002/9781119942283" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Rubin, D. B. <em><font color="#428bca">Multlipe Imputation for Nonresponse in Surveys</font></em>. Hoboken, NJ, USA: Wiley, 1987. ISBN: 9780471655740.</cite></pref></li>
</ul>
</div>
</div>
<!-- Journal articles -->
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#mi_journals">
Journal articles (31)
</button>
<div id="mi_journals" class="collapse">
<ul>
<li><pref><cite>Abayomi, K., A. Gelman, and M. Levy. <font color="#428bca">Diagnostics for multivariate imputations</font>. In: <em>Journal of the Royal Statistical Society, Series C (Applied Statistics)</em> 57.3 (2008), pp. 273-291.</cite></pref>
<div>
<a href="https://doi.org/10.1111/j.1467-9876.2007.00613.x" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Audigier, V., F. Husson, and J. Josse. <font color="#428bca">Multiple imputation for continuous variables using a Bayesian principal component analysis</font>. In: <em>Journal of Statistical Computation and Simulation</em> 86.11 (2015), pp. 2140-2156.</cite></pref>
<div>
<a href="https://doi.org/10.1080/00949655.2015.1104683" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Audigier, V., F. Husson, and J. Josse. <font color="#428bca">MIMCA: multiple imputation for categorical variables with multiple correspondence analysis</font>. In: <em>Statistics and Computing</em> 27.2 (2016), pp. 1-18. eprint: 1505.08116.</cite></pref>
<div>
<a href="https://doi.org/10.1007/s11222-016-9635-4" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Carpenter, J. R., M. G. Kenward, and S. Vansteelandt. <font color="#428bca">A comparison of multiple imputation and doubly robust estimation for analyses with missing data</font>. In: <em>Journal of the Royal Statistical Society: Series A (Statistics in Society)</em> 169.3 (2006), pp. 571–584.</cite></pref>
<div>
<a href="https://doi.org/10.1111/j.1467-985X.2006.00407.x" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Collins, L. M., J. L. Schafer, and K. Chi-Ming. <font color="#428bca">A comparison of inclusive and restrictive strategies in modern missing data procedures</font>. In: <em>Psychological Methods</em> 6.4 (2007), pp. 330-351.</cite></pref>
<div>
<a href="https://doi.org/10.1037/1082-989X.6.4.330" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Erler, N. S., D. Rizopoulos, and E. M. Lesaffre. <font color="#428bca">JointAI: joint analysis and imputation of incomplete data in R</font>. In: <em>arXiv preprint</em> (2019).</cite></pref>
<div>
<a href="https://arxiv.org/abs/1907.10867" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Fay, R. E. <font color="#428bca">Alternative paradigms for the analysis of imputed survey data</font>. In: <em>Journal of the American Statistical Association</em> 91.434 (1996), pp. 490-498.</cite></pref>
<div>
<a href="https://doi.org/10.1080/01621459.1996.10476909" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Gelman, A., G. King, and C. Liu. <font color="#428bca">Not asked and not answered: Multiple imputation for multiple surveys</font>. In: <em>Journal of the American Statistical Association</em> 93.443 (1998), pp. 846–857.</cite></pref>
<div>
<a href="https://doi.org/10.1080/01621459.1998.10473737" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Gelman, A., I. van Mechelen, G. Verbeke, et al. <font color="#428bca">Multiple Imputation for Model Checking: Completed-Data Plots with Missing and Latent Data</font>. In: <em>Biometrics</em> 61.1 (2005), pp. 74–85.</cite></pref>
<div>
<a href="https://doi.org/10.1111/j.0006-341X.2005.031010.x" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Graham, J. W., A. E. Olchowski, and T. E. Gilreath. <font color="#428bca">How many imputations are really needed? Some practical clarifications of multiple imputation theory</font>. In: <em>Prevention Science</em> 8.3 (2007), pp. 206-213.</cite></pref>
<div>
<a href="https://doi.org/10.1007/s11121-007-0070-9" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Honaker, J., G. King, and M. Blackwell. <font color="#428bca">Amelia II: a program for missing data</font>. In: <em>Journal of Statistical Software</em> 45.7 (2011). eprint: arXiv:1501.0228.</cite></pref>
<div>
<a href="https://doi.org/10.18637/jss.v045.i07" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Imbert, A., A. Valsesia, C. Le Gall, et al. <font color="#428bca">Multiple hot-deck imputation for network inference from RNA sequencing data</font>. In: <em>Bioinformatics</em> 34.10 (2018), pp. 1726-1732.</cite></pref>
<div>
<a href="https://doi.org/10.1093/bioinformatics/btx819" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Josse, J., J. Pagès, and F. Husson. <font color="#428bca">Multiple imputation in principal component analysis</font>. In: <em>Advances in Data Analysis and Classification</em> 5.3 (2011), pp. 231-246.</cite></pref>
<div>
<a href="https://doi.org/10.1007/s11634-011-0086-7" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Josse, J. and F. Husson. <font color="#428bca">Handling missing values in exploratory multivariate data analysis methods</font>. In: <em>Journal de la Société Française de Statistique</em> 153.2 (2012), pp. 79-99.</cite></pref>
<div>
<a href="http://publications-sfds.fr/ojs/index.php/J-SFdS/article/view/122/112" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Josse, J. and F. Husson. <font color="#428bca">missMDA: a package for handling missing values in multivariate data analysis</font>. In: <em>Journal of Statistical Software</em> 70.1 (2016), pp. 1-31.</cite></pref>
<div>
<a href="https://doi.org/10.18637/jss.v070.i01" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Kropko, J., B. Goodrich, A. Gelman, et al. <font color="#428bca">Multiple Imputation for Continuous and Categorical Data: Comparing Joint Multivariate Normal and Conditional Approaches</font>. In: <em>Political Analysis</em> 22.4 (2014), pp. 497–519.</cite></pref>
<div>
<a href="https://doi.org/10.1093/pan/mpu007" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Larose, C., D. K. Dey, and O. Harel. <font color="#428bca">The impact of missing values on different measures of uncertainty</font>. In: <em>Statistica Sinica</em> 29.2 (2019), pp. 551–566.</cite></pref>
<div>
<a href="https://doi.org/10.5705/ss.202016.0073" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Murray, J. S. and J. P. Reiter. <font color="#428bca">Multiple Imputation of Missing Categorical and Continuous Values via Bayesian Mixture Models With Local Dependence</font>. In: <em>Journal of the American Statistical Association</em> 111.516 (2016), pp. 1466-1479.</cite></pref>
<div>
<a href="https://doi.org/10.1080/01621459.2016.1174132" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Quartagno, M. and J. R. Carpenter. <font color="#428bca">Multiple imputation for discrete data: Evaluation of the joint latent normal model</font>. In: <em>Biometrical Journal</em> 61.4 (2019), pp. 1003–1019.</cite></pref>
<div>
<a href="https://doi.org/10.1002/bimj.201800222" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Robins, J. M. and N. Wang. <font color="#428bca">Inference for imputation estimators</font>. In: <em>Biometrika</em> 87.1 (2000), pp. 113-124.</cite></pref>
<div>
<a href="https://www.jstor.org/stable/2673565" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Rubin, D. B. <font color="#428bca">Multiple imputation after 18+ years</font>. In: <em>Journal of the American Statistical Association</em> 91.434 (2012), pp. 473-489.</cite></pref>
<div>
<a href="https://doi.org/10.1080/01621459.1996.10476908" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Schafer, J. L. and M. K. Olsen. <font color="#428bca">Multiple Imputation for multivariate missing-data problems: a data analyst’s perspective</font>. In: <em>Multivariate Behavioral Research</em> 33.4 (1998), pp. 545-571.</cite></pref>
<div>
<a href="https://doi.org/10.1207/s15327906mbr3304_5" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Schafer, J. L. <font color="#428bca">Multiple imputation: a primer</font>. In: <em>Statistical Methods in Medical Research</em> 8.1 (1999), pp. 3-15.</cite></pref>
<div>
<a href="https://doi.org/10.1191/096228099671525676" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Stuart, E. A., M. Azur, C. Frangakis, et al. <font color="#428bca">Multiple imputation with large data sets: a case study of the children’s mental health initiative</font>. In: <em>American Journal of Epidemiology</em> 169.9 (2009), pp. 1133-1139.</cite></pref>
<div>
<a href="https://doi.org/10.1093/aje/kwp026" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Su, Y. S., A. Gelman, J. Hill, et al. <font color="#428bca">Multiple imputation with diagnostics (mi) in R: opening windows into the black box</font>. In: <em>Journal of Statistical Software</em> 45 (2011), p. 2.</cite></pref>
<div>
<a href="https://doi.org/10.18637/jss.v045.i02" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Buuren, S. van, J. P. L. Brand, C. G. M. Groothuis-Oudshoorn, et al. <font color="#428bca">Fully conditional specification in multivariate imputation</font>. In: <em>Journal of Statistical Computation and Simulation</em> 76.12 (2006), pp. 1049-1064.</cite></pref>
<div>
<a href="https://doi.org/10.1080/10629360600810434" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Buuren, S. van and K. Groothuis-Oudshoorn. <font color="#428bca">MICE: multivariate imputation by chained equations in R</font>. In: <em>Journal of Statistical Software</em> 45 (2011), p. 3. eprint: NIHMS150003.</cite></pref>
<div>
<a href="https://doi.org/10.18637/jss.v045.i03" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Buuren, S. van. <font color="#428bca">Multiple imputation of discrete and continuous data by fully conditional specification</font>. In: <em>Statistical Methods in Medical Research</em> 16 (2007), pp. 219-242.</cite></pref>
<div>
<a href="https://doi.org/10.1177/0962280206074463" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Voillet, V., P. Besse, L. Liaubet, et al. <font color="#428bca">Handling missing rows in multi-omics data integration: multiple imputation in multiple factor analysis framework</font>. In: <em>BMC Bioinformatics</em> 17.402 (2016). Forthcoming.</cite></pref>
<div>
<a href="https://doi.org/10.1186/s12859-016-1273-5" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Wang, N. and J. M. Robins. <font color="#428bca">Large-sample theory for parametric multiple imputation procedures</font>. In: <em>Biometrika</em> 85.4 (1998), pp. 935–948.</cite></pref>
<div>
<a href="https://doi.org/10.1093/biomet/85.4.935" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Xie, X. and X. L. Meng. <font color="#428bca">Dissecting multiple imputation from a multi-phase inference perspective: what happens when God’s, imputer’s and analyst’s models are uncongenial?</font> In: <em>Statistica Sinica</em> 27.4 (2017), pp. 1485–1594.</cite></pref>
<div>
<a href="https://doi.org/10.5705/ss.2014.067" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
</ul>
</div>
</div>
<!-- Conference papers -->
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#mi_conf">
Conference papers (2)
</button>
<div id="mi_conf" class="collapse">
<ul>
<li><pref><cite>Gondara, L. and K. Wang. <font color="#428bca">MIDA: Multiple Imputation using Denoising Autoencoders</font>. In: <em>Proceedings of the 22nd Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD 2018)</em>. (Jun. 03, 2018-Jun. 06, 2018). Ed. by D. Phung, V. Tseng, G. Webb, B. Ho, M. Ganji and L. Rashidi. Lecture Notes in Computer Science. Springer International Publishing, 2018, pp. 260-272. ISBN: 3319930404.</cite></pref>
<div>
<a href="https://doi.org/10.1007/978-3-319-93040-4_21" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a><a href="https://arxiv.org/abs/1705.02737" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Muzellec, B., J. Josse, C. Boyer, et al. <font color="#428bca">Missing Data Imputation using Optimal Transport</font>. In: <em>International Conference on Machine Learning</em>. PMLR. 2020, pp. 7130–7140.</cite></pref></li>
</ul>
</div>
</div>
<!-- Reports, theses, etc. -->
<p><br></p>
</div>
</div>
</div>
<!------------------------- Machine Learning ------------------------->
<div class="panel panel-default">
<div id="h_tree_dl" class="panel-heading" role="tab">
<h4 class="panel-title">
<a role="button" data-toggle="collapse" data-parent="#accordion" href="#tree_dl" aria-expanded="false" aria-controls="tree_dl">Machine Learning</a>
</h4>
</div>
<div id="tree_dl" class="panel-collapse collapse in" role="tabpanel" aria-labelledby="h_tree_dl">
<div class="panel-body">
<p align="justify">
The field of machine learning being dependent on the availability of (good) training data, it is – in most real-world applications – necessarily facing the issue of missing data. Hence there has been an increasing attention to how to handle missing data, in the features and the output, in order to <em>learn</em> accurately from the data.
</p>
<p><font size="+1"><b>Supervised learning</b></font></p>
<p align="justify">
Methods to deal with supervised learning (predict as well as possible an outcome) with missing values in the covariates are really different from methods for inference with missing values (estimating parameters).
</p>
</p>
<!-- Books and book chapters -->
<!-- Journal articles -->
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#algoml_journals">
Journal articles (1)
</button>
<div id="algoml_journals" class="collapse">
<ul>
<li><pref><cite>Ma, A. and D. Needell. <font color="#428bca">Stochastic Gradient Descent for Linear Systems with Missing Data</font>. In: <em>Numerical Mathematics: Theory, Methods and Applications</em> 12.1 (2017), pp. 1-20.</cite></pref>
<div>
<a href="https://doi.org/10.4208/nmtma.OA-2018-0066" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
</ul>
</div>
</div>
<!-- Conference papers -->
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#algoml_conf">
Conference papers (4)
</button>
<div id="algoml_conf" class="collapse">
<ul>
<li><pref><cite>Ipsen, N. B., P. Mattei, and J. Frellsen. <font color="#428bca">not-MIWAE: Deep generative modelling with missing not at random data</font>. In: <em>arXiv preprint</em> (2020).</cite></pref> <pref><cite>Ipsen, N., P. Mattei, and J. Frellsen. <font color="#428bca">How to deal with missing data in supervised deep learning?</font> In: <em>ICML Workshop on the Art of Learning with Missing Values (Artemiss)</em>. 2020.</cite></pref>
<div>
<a href="https://hal.inria.fr/hal-03044144/" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Le Morvan, M., N. Prost, J. Josse, et al. <font color="#428bca">Linear predictor on linearly-generated data with missing values: non consistency and solutions</font>. In: <em>Proceedings of Machine Learning Research</em>. Ed. by -. Vol. 108. Proceedings of Machine Learning Research. 2020, p. 3165–3174. eprint: 2002.00658v2.</cite></pref>
<div>
<a href="http://proceedings.mlr.press/v108/morvan20a/morvan20a.pdf" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Le Morvan, M., J. Josse, T. Moreau, et al. <font color="#428bca">NeuMiss networks: differentiable programming for supervised learning with missing values.</font> In: <em>Advances in Neural Information Processing Systems, 33</em>. (Dec. 2020). Ed. by -. IEEE, 2020. eprint: 2007.01627v4.</cite></pref>
<div>
<a href="https://proceedings.neurips.cc/paper/2020/file/42ae1544956fbe6e09242e6cd752444c-Paper.pdf" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Sportisse, A., C. Boyer, A. Dieuleveut, et al. <font color="#428bca">Debiasing Averaged Stochastic Gradient Descent to handle missing values</font>. In: <em>Advances in Neural Information Processing Systems, 33</em>. (Dec. 2020). Ed. by -. IEEE, 2020. eprint: 2002.09338v2.</cite></pref>
<div>
<a href="https://arxiv.org/abs/2002.09338" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
</ul>
</div>
</div>
<!-- Reports, theses, etc. -->
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#algoml_misc">
Reports, theses, etc. (1)
</button>
<div id="algoml_misc" class="collapse">
<ul>
<li><pref><cite>Le Morvan, M., J. Josse, E. Scornet, et al. <font color="#428bca">What’s a good imputation to predict with missing values?</font> 2021.</cite></pref>
<div>
<a href="https://arxiv.org/pdf/2106.00311.pdf" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
</ul>
</div>
</div>
<p><br></p>
<p><font size="+1"><b>Unsupervised learning</b></font></p>
<p align="justify">
Methods have been suggested to perform clustering with missing values (k-means, mixture models) as well as dimensionality reduction with missing values (PCA).
</p>
</p>
<!-- Books and book chapters -->
<!-- Journal articles -->
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#clust_journals">
Journal articles (5)
</button>
<div id="clust_journals" class="collapse">
<ul>
<li><pref><cite>Brinis, S., C. Traina, and A. J. Traina. <font color="#428bca">Hollow-tree: a metric access method for data with missing values</font>. In: <em>Journal of Intelligent Information Systems</em> (2019), pp. 1–28.</cite></pref>
<div>
<a href="https://doi.org/10.1007/s10844-019-00567-8" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Hunt, L. and M. Jorgensen. <font color="#428bca">Mixture model clustering for mixed data with missing information</font>. In: <em>Computational Statistics &amp; Data Analysis</em> 41.3-4 (2003), pp. 429–440.</cite></pref>
<div>
<a href="https://doi.org/https://doi.org/10.1016/S0167-9473(02)00190-1" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Chi, J. T., E. C. Chi, and R. G. Baraniuk. <font color="#428bca">k-pod: A method for k-means clustering of missing data</font>. In: <em>The American Statistician</em> 70.1 (2016), pp. 91–99.</cite></pref>
<div>
<a href="https://doi.org/https://doi.org/10.1080/00031305.2015.1086685" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Josse, J., M. Chavent, B. Liquet, et al. <font color="#428bca">Handling missing values with regularized iterative multiple correspondance analysis</font>. In: <em>Journal of Classification</em> 29.1 (2012), pp. 91-116.</cite></pref>
<div>
<a href="https://doi.org/10.1007/s00357-012-9097-0" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Miao, W. and E. J. Tchetgen Tchetgen. <font color="#428bca">Identification and inference with nonignorable missing covariate data</font>. In: <em>Statistica Sinica</em> 28.4 (2018), pp. 2049–2067.</cite></pref>
<div>
<a href="https://doi.org/10.5705/ss.202016.0322" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
</ul>
</div>
</div>
<!-- Conference papers -->
<!-- Reports, theses, etc. -->
<p><br></p>
<p><font size="+1"><b>Trees and forests</b></font></p>
<p align="justify">
Decision trees are models based on recursive executions of elementary rules. This architecture grants them a variety of simple options to deal with missing values, without requiring prior imputation. A popular class of decision tree models is called <em>random trees</em> (or more generally <em>random forests</em>) and allows data analyses such as causal inference in the presence of missing values without the need of having to impute these missing values.
</p>
<!-- Books and book chapters -->
<!-- Journal articles -->
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#tree_journals">
Journal articles (12)
</button>
<div id="tree_journals" class="collapse">
<ul>
<li><pref><cite>Beaulac, C. and J. S. Rosenthal. <font color="#428bca">BEST: A decision tree algorithm that handles missing values</font>. In: <em>arXiv preprint</em> (2018). eprint: 1804.10168.</cite></pref>
<div>
<a href="https://arxiv.org/pdf/1804.10168.pdf" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Bertsimas, D., C. Pawlowski, and Y. D. Zhuo. <font color="#428bca">From predictive methods to missing data imputation: an optimization approach</font>. In: <em>The Journal of Machine Learning Research</em> 18.1 (2017), pp. 7133–7171.</cite></pref></li>
<li><pref><cite>Ding, Y. and J. S. Simonoff. <font color="#428bca">An investigation of missing data methods for classification trees applied to binary response data</font>. In: <em>Journal of Machine Learning Research</em> 11.1 (2010), pp. 131-170.</cite></pref>
<div>
<a href="http://www.jmlr.org/papers/v11/ding10a.html" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Hothorn, T., K. Hornik, and A. Zeileis. <font color="#428bca">Unbiased Recursive Partitioning: A Conditional Inference Framework</font>. In: <em>Journal of Computational and Graphical Statistics</em> 15.3 (2012), pp. 651-674.</cite></pref>
<div>
<a href="https://doi.org/10.1198/106186006X133933" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Josse, J., N. Prost, E. Scornet, et al. <font color="#428bca">On the consistency of supervised learning with missing values</font>. In: <em>arXiv preprint</em> (2019). arXiv: 1902.06931 [stat.ML].</cite></pref>
<div>
<a href="https://arxiv.org/abs/1902.06931" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Kapelner, A. and J. Bleich. <font color="#428bca">Prediction with missing data via Bayesian additive regression trees</font>. In: <em>Canadian Journal of Statistics</em> 43.2 (2015), pp. 224-239.</cite></pref>
<div>
<a href="https://doi.org/10.1002/cjs.11248" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a><a href="https://arxiv.org/abs/1306.0618v3" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Khosravi, P., A. Vergari, Y. Choi, et al. <font color="#428bca">Handling missing data in decision trees: A probabilistic approach</font>. In: <em>arXiv preprint arXiv:2006.16341</em> (2020).</cite></pref></li>
<li><pref><cite>Rahman, G. and Z. Islam. <font color="#428bca">Missing value imputation using decision trees and decision forests by splitting and merging records: Two novel techniques</font>. In: <em>Knowledge-Based Systems</em> 53 (2013), pp. 51–65.</cite></pref>
<div>
<a href="https://doi.org/10.1016/j.knosys.2013.08.023" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a><a href="http://www.sciencedirect.com/science/article/pii/S0950705113002591" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Stekhoven, D. J. and P. Bühlmann. <font color="#428bca">Missforest-non-parametric missing value imputation for mixed-type data</font>. In: <em>Bioinformatics</em> 28.1 (2012), pp. 112-118. eprint: 1105.0828.</cite></pref>
<div>
<a href="https://doi.org/10.1093/bioinformatics/btr597" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Strobl, C., A. L. Boulesteix, and T. Augustin. <font color="#428bca">Unbiased split selection for classification trees based on the Gini Index</font>. In: <em>Computational Statistics &amp; Data Analysis</em> 52.1 (2007), pp. 483-501.</cite></pref>
<div>
<a href="https://doi.org/10.1016/j.csda.2006.12.030" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Tierney, N. J., F. A. Harden, M. J. Harden, et al. <font color="#428bca">Using decision trees to understand structure in missing data</font>. In: <em>BMJ Open</em> 5.6 (2015), p. e007450.</cite></pref>
<div>
<a href="https://doi.org/10.1136/bmjopen-2014-007450" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Twala, B. E. T. H., M. C. Jones, and D. J. Hand. <font color="#428bca">Good methods for coping with missing data in decision trees</font>. In: <em>Pattern Recognition Letters</em> 29.7 (2008), pp. 950-956.</cite></pref>
<div>
<a href="https://doi.org/10.1016/j.patrec.2008.01.010" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
</ul>
</div>
</div>
<!-- Conference papers -->
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#tree_conf">
Conference papers (1)
</button>
<div id="tree_conf" class="collapse">
<ul>
<li><pref><cite>Chen, T. and C. Guestrin. <font color="#428bca">XGBoost: A Scalable Tree Boosting System</font>. In: <em>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em>. (Aug. 13, 2016-Aug. 17, 2016). Ed. by -. New York, NY, USA: ACM, 2016, pp. 785-794. ISBN: 0450342322.</cite></pref>
<div>
<a href="https://doi.org/10.1145/2939672.2939785" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
</ul>
</div>
</div>
<!-- Reports, theses, etc. -->
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#tree_misc">
Reports, theses, etc. (1)
</button>
<div id="tree_misc" class="collapse">
<ul>
<li><pref><cite>Rieger, A., T. Hothorn, and C. Strobl. <em><font color="#428bca">Random forests with missing values in the covariates</font></em>. Tech. rep. 79. University of Munich, Department of Statistics, 2010.</cite></pref>
<div>
<a href="https://epub.ub.uni-muenchen.de/11481/1/techreport.pdf" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
</ul>
</div>
</div>
<p><br></p>
<p><font size="+1"><b>Deep Learning</b></font></p>
<p align="justify">
The advance and success of <em>(deep) neural networks</em> in many research and application areas such as computer vision and natural language processing has also re-discovered the problem of handling missing values. Indeed the question of training neural networks on incomplete data has been considered even before the latest rise of deep learning and is considered to be essential due to the impact of missingness on the feasibility and quality of various learning problems.
</p>
<!-- Books and book chapters -->
<!-- Journal articles -->
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#dl_journals">
Journal articles (6)
</button>
<div id="dl_journals" class="collapse">
<ul>
<li><pref><cite>Bianchi, F. M., L. Livi, K. Ø. Mikalsen, et al. <font color="#428bca">Learning representations of multivariate time series with missing data</font>. In: <em>Pattern Recognition</em> 96 (2019), p. 106973.</cite></pref>
<div>
<a href="https://doi.org/10.1016/j.patcog.2019.106973" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Ipsen, N. B., P. Mattei, and J. Frellsen. <font color="#428bca">not-MIWAE: Deep generative modelling with missing not at random data</font>. In: <em>arXiv preprint</em> (2020).</cite></pref>
<div>
<a href="https://arxiv.org/abs/2006.12871" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Sharpe, P. K. and R. J. Solly. <font color="#428bca">Dealing with missing values in neural network-based diagnostic systems</font>. In: <em>Neural Computing &amp; Applications</em> 3.2 (1995), pp. 73-77.</cite></pref>
<div>
<a href="https://doi.org/10.1007/BF01421959" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Śmieja, M., Ł. Struski, J. Tabor, et al. <font color="#428bca">Processing of missing data by neural networks</font>. In: <em>Computing Research Repository</em> abs/1805.07405 (2018). eprint: 1805.07405.</cite></pref>
<div>
<a href="https://arxiv.org/abs/1805.07405" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Sovilj, D., E. Eirola, Y. Miche, et al. <font color="#428bca">Extreme learning machine for missing data using multiple imputations</font>. In: <em>Neurocomputing</em> 174.A (2016), pp. 220-231.</cite></pref>
<div>
<a href="https://doi.org/10.1016/j.neucom.2015.03.108" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Zhang, H., P. Xie, and E. Xing. <font color="#428bca">Missing Value Imputation Based on Deep Generative Models</font>. In: <em>Computing Research Repository</em> abs/1808.01684 (2018).</cite></pref>
<div>
<a href="https://arxiv.org/abs/1808.01684" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
</ul>
</div>
</div>
<!-- Conference papers -->
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#dl_conf">
Conference papers (11)
</button>
<div id="dl_conf" class="collapse">
<ul>
<li><pref><cite>Bengio, Y. and F. Gingras. <font color="#428bca">Recurrent neural networks for missing or asynchronous data</font>. In: <em>Proceedings of the 8th International Conference on Neural Information Processing Systems</em>. (Nov. 27, 1995-Dec. 02, 1995). Ed. by -. Cambridge, MA, USA: MIT Press, 1995, pp. 395-401.</cite></pref>
<div>
<a href="http://papers.nips.cc/paper/1126-recurrent-neural-networks-for-missing-or-asynchronous-data.pdf" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Biessmann, F., D. Salinas, S. Schelter, et al. <font color="#428bca">Deep" Learning for Missing Value Imputation in Tables with Non-Numerical Data</font>. In: <em>Proceedings of the 27th ACM International Conference on Information and Knowledge Management</em>. Ed. by -. CIKM ’18. Torino, Italy: ACM, 2018, pp. 2017–2025. ISBN: 978-1-4503-6014-2.</cite></pref>
<div>
<a href="https://doi.org/10.1145/3269206.3272005" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a><a href="http://doi.acm.org/10.1145/3269206.3272005" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Gondara, L. and K. Wang. <font color="#428bca">MIDA: Multiple Imputation using Denoising Autoencoders</font>. In: <em>Proceedings of the 22nd Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD 2018)</em>. (Jun. 03, 2018-Jun. 06, 2018). Ed. by D. Phung, V. Tseng, G. Webb, B. Ho, M. Ganji and L. Rashidi. Lecture Notes in Computer Science. Springer International Publishing, 2018, pp. 260-272. ISBN: 3319930404.</cite></pref>
<div>
<a href="https://doi.org/10.1007/978-3-319-93040-4_21" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a><a href="https://arxiv.org/abs/1705.02737" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Goodfellow, I., M. Mirza, A. Courville, et al. <font color="#428bca">Multi-Prediction Deep Boltzmann Machines</font>. In: <em>Proceedings of the 26th International Conference on Neural Information Processing Systems</em>. (Dec. 05, 2013-Dec. 10, 2013). Ed. by C. Burges, L. Bottou, M. Welling, Z. Ghahramani and K. Weinberger. Advances in Neural Information Processing Systems 26. Curran Associates, Inc., 2013, pp. 548–556.</cite></pref>
<div>
<a href="http://papers.nips.cc/paper/5024-multi-prediction-deep-boltzmann-machines.pdf" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Ipsen, N. B., P. Mattei, and J. Frellsen. <font color="#428bca">not-MIWAE: Deep generative modelling with missing not at random data</font>. In: <em>arXiv preprint</em> (2020).</cite></pref> <pref><cite>Ipsen, N., P. Mattei, and J. Frellsen. <font color="#428bca">How to deal with missing data in supervised deep learning?</font> In: <em>ICML Workshop on the Art of Learning with Missing Values (Artemiss)</em>. 2020.</cite></pref>
<div>
<a href="https://hal.inria.fr/hal-03044144/" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Mattei, P. and J. Frellsen. <font color="#428bca">MIWAE: Deep generative modelling and imputation of incomplete data sets</font>. In: <em>Proceedings of the 36th International Conference on Machine Learning</em>. Vol. 97. Proceedings of Machine Learning Research. Kamalika Chaudhuri and Ruslan Salakhutdinov, 2019, pp. 4413–4423.</cite></pref>
<div>
<a href="http://proceedings.mlr.press/v97/mattei19a.html" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Le Morvan, M., J. Josse, T. Moreau, et al. <font color="#428bca">NeuMiss networks: differentiable programming for supervised learning with missing values.</font> In: <em>Advances in Neural Information Processing Systems, 33</em>. (Dec. 2020). Ed. by -. IEEE, 2020. eprint: 2007.01627v4.</cite></pref>
<div>
<a href="https://proceedings.neurips.cc/paper/2020/file/42ae1544956fbe6e09242e6cd752444c-Paper.pdf" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Nowicki, R. K., R. Scherer, and L. Rutkowski. <font color="#428bca">Novel rough neural network for classification with missing data</font>. In: <em>21st International Conference on Methods and Models in Automation and Robotics (MMAR)</em>. (Sep. 29, 2016-Sep. 01, 2016). Ed. by -. IEEE, 2016, pp. 820–825.</cite></pref>
<div>
<a href="https://doi.org/10.1109/MMAR.2016.7575243" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Tran, L., X. Liu, J. Zhou, et al. <font color="#428bca">Missing Modalities Imputation via Cascaded Residual Autoencoder</font>. In: <em>2017 IEEE Conference on Computer Vision and PAttern Recognition (CVPR)</em>. (Jul. 21, 2017-Jul. 26, 2017). Ed. by -. IEEE, 2017, pp. 4971-4980.</cite></pref>
<div>
<a href="https://doi.org/10.1109/CVPR.2017.528" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Yoon, J., J. Jordon, and M. van der Schaar. <font color="#428bca">GAIN: Missing Data Imputation using Generative Adversarial Nets</font>. In: <em>Proceedings of the 35th International Conference on Machine Learning</em>. (Jul. 10, 2018-Jul. 15, 2018). Ed. by J. Dy and A. Krause. Vol. 80. Proceedings of Machine Learning Research. Stockholmsmässan, Stockholm Sweden: PMLR, 2018, pp. 5689–5698.</cite></pref>
<div>
<a href="http://proceedings.mlr.press/v80/yoon18a.html" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Yoon, S. and S. Sull. <font color="#428bca">GAMIN: Generative Adversarial Multiple Imputation Network for Highly Missing Data</font>. In: <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 2020, pp. 8456–8464.</cite></pref>
<div>
<a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Yoon_GAMIN_Generative_Adversarial_Multiple_Imputation_Network_for_Highly_Missing_Data_CVPR_2020_paper.html" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
</ul>
</div>
</div>
<!-- Reports, theses, etc. -->
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#dl_misc">
Reports, theses, etc. (1)
</button>
<div id="dl_misc" class="collapse">
<ul>
<li><pref><cite>Londschien, M., S. Kovács, and P. Bühlmann. <em><font color="#428bca">Change point detection for graphical models in presence of missing values</font></em>. 2019. arXiv: 1907.05409 [stat.ML].</cite></pref></li>
</ul>
</div>
</div>
<p><br></p>
</div>
</div>
</div>
<!------------------------- MECHANISMS AND IDENTIFIABILITY ------------------------->
<div class="panel panel-default">
<div id="h_mnar" class="panel-heading" role="tab">
<h4 class="panel-title">
<a role="button" data-toggle="collapse" data-parent="#accordion" href="#mnar" aria-expanded="false" aria-controls="mnar">Missing values mechanisms and identifiability</a>
</h4>
</div>
<div id="mnar" class="panel-collapse collapse in" role="tabpanel" aria-labelledby="h_mnar">
<div class="panel-body">
<p align="justify">
As mentioned in the above sections, it is necessary to make assumptions on the <em>mechanism generating the missing values</em> or <em>response mechanism</em> in order to work with missing values. Broadly speaking, these assumptions indicate how much the missingness is related to the data itself. The assumptions made on the mechanism impact further steps in the data analysis (since some types of missingness can induce a bias on the analysis results) and are therefore crucial for valid analyses of data in the presence of missing values.
</p>
<p align="justify">
More formally, both <span class="math inline">\(X\)</span> and <span class="math inline">\(R\)</span> are modeled as random variables and the response mechanism is defined as the conditional distribution of <span class="math inline">\(R\)</span> given <span class="math inline">\(X\)</span>, <span class="math inline">\(\mathbb{P}_R(R|X)\)</span>. This distribution can depend on some parameter <span class="math inline">\(\phi\)</span> so that we have <span class="math inline">\(\mathbb{P}_R(R|X;\phi)\)</span>. Little and Rubin (2002) defined three main categories of missing values depending on the form of the conditional distribution <span class="math inline">\(\mathbb{P}_R\)</span>:
</p>
<ul>
<li><p align="justify">
<p>Missing completely at random (MCAR): The missingness does not depend on the variables <span class="math inline">\(X=(X^\mathrm{obs},X^\mathrm{mis})\)</span>, denoting the observed variables and the missing ones as <span class="math inline">\(X^\mathrm{obs}\)</span> and <span class="math inline">\(X^\mathrm{mis}\)</span> respectively i.e.</p>
</p>
<p><span class="math display">\[\mathbb{P}_R(R|X^\mathrm{obs},X^\mathrm{mis};\phi) = \mathbb{P}_R(R;\phi), \forall \phi\]</span></p></li>
<li><p align="justify">
<p>Missing at random (MAR): The missingness depends only on the observed variables <span class="math inline">\(X_{obs}\)</span>, i.e.</p>
</p>
<p><span class="math display">\[\mathbb{P}_R(R|X^\mathrm{obs},X^\mathrm{mis};\phi) = \mathbb{P}_R(R|X^\mathrm{obs};\phi), \forall \phi,X^\mathrm{mis}\]</span>
<!-- <p align="justify">or alternatively $f(R|X^1;\psi) = f(R|X^2;\psi)$ for all $X^1 = (X^1_{obs},X^1_{mis})$ and $X^2 = (X^2_{obs},X^2_{mis})$ such that $X^1_{obs} = X^2_{obs}$.</p>  --></p></li>
<li><p align="justify">
<p>Missing not at random (MNAR): The missingness is said MNAR in all other cases, i.e. the missingness depends on the missing values and potentially also on the observed values.</p></li>
</ul>
<!-- i.e. $$\mathbb{P}_R(R|X;\phi) \neq \mathbb{P}_R(R|X^\mathrm{obs};\phi), \forall \phi, .$$  -->
To understand this definition, take the example of alcohol consumption: alcoholics are less inclined to reveal their alcohol consumption, therefore the probability of missing information on the alcohol consumption depends on the amount of consumption itself. Another simple example is the information on income or wealth which is missing more often for individuals of very high or very low income.
</p>
<p align="justify">
Note that MCAR is a special case of MAR and that these three categories are of increasing complexity with a large gap between the second and third. Indeed, most more or less generic methods which have been proposed in the last few decades are suited for data that is MAR. The case MNAR requires different techniques and further assumptions.
</p>
<p align="justify">
Note that Little and Rubin (2002) consider these three categories as <em>really missing values</em> as opposed to <em>not really missing values</em> where, in the case of categorical data, the missingness rather constitutes an additional category (for instance in a questionnaire with multiple choice answers, a participant can leave out a question because the category he wants to choose is not among the given choices).
</p>
<p align="justify">
<p>Another – maybe complementary – approach to consider and study different missing values mechanisms and problems consists in using graphical models, for instance <em>missingness graphs</em> or <em>m-graphs</em> (Mohan et al., 2013). These allow to represent multivariate dependencies and to study identifiability or recoverability for different (estimation or prediction) problems.</p>
<p align="justify">
Finally, another line of research considers the occurrence of missing values beforehand and addresses the question of how to anticipate or control the occurrence of missing values in a study design.
</p>
<!-- Books and book chapters -->
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#mnar_books">
Books and book chapters (1)
</button>
<div id="mnar_books" class="collapse">
<ul>
<li><pref><cite>Wainer, H., ed. <em><font color="#428bca">Drawing Inferences from Self-Selected Samples</font></em>. New York, NY, USA: Springer, 1986.</cite></pref></li>
</ul>
</div>
</div>
<!-- Journal articles -->
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#mnar_journals">
Journal articles (45)
</button>
<div id="mnar_journals" class="collapse">
<ul>
<li><pref><cite>Albert, P. S. and D. A. Follmann. <font color="#428bca">Modeling repeated count data subject to informative dropout</font>. In: <em>Biometrics</em> 56.3 (2000), pp. 667-677.</cite></pref>
<div>
<a href="https://doi.org/10.1111/j.0006-341X.2000.00667.x" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Chen, Y. and M. Sadinle. <font color="#428bca">Nonparametric Pattern-Mixture Models for Inference with Missing Data</font>. In: <em>arXiv preprint</em> (2019). arXiv: 1904.11085 [stat.ME].</cite></pref>
<div>
<a href="https://arxiv.org/pdf/1904.11085.pdf" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Diggle, P. and M. G. Kenward. <font color="#428bca">Informative drop-out in longitudinal data analysis</font>. In: <em>Journal of the Royal Statistical Society, Series C (Applied Statistics)</em> 43.1 (1994), pp. 49-93.</cite></pref>
<div>
<a href="https://doi.org/10.2307/2986113" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Fang, F., J. Zhao, and J. Shao. <font color="#428bca">Imputation-based adjusted score equations in generalized linear models with nonignorable missing covariate values</font>. In: <em>Statistica Sinica</em> 28.4 (2018), pp. 1677–1701.</cite></pref>
<div>
<a href="https://doi.org/10.5705/ss.202015.0437" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Follmann, D. and M. Wu. <font color="#428bca">An approximate generalized linear model with random effects for informative missing data</font>. In: <em>Biometrics</em> 51.1 (1995), pp. 151-168.</cite></pref>
<div>
<a href="https://doi.org/10.2307/2533322" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Gad, A. M. and N. M. M. Darwish. <font color="#428bca">A shared parameter model for longitudinal data with missing values</font>. In: <em>American Journal of Applied Mathematics and Statistics</em> 1.2 (2013), pp. 30-35.</cite></pref>
<div>
<a href="http://pubs.sciepub.com/ajams/1/2/3" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Heckman, J. J. <font color="#428bca">The common structure of statistical models of truncation, sample selection and limited dependent variables and a simple estimator for such models</font>. In: <em>Annals of Economic and Social Measurement</em> 5.4 (1976), pp. 475-492.</cite></pref>
<div>
<a href="http://ideas.repec.org/h/nbr/nberch/10491.html" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Ibrahim, J. G., M. Chen, and S. R. Lipsitz. <font color="#428bca">Missing responses in generalised linear mixed models when the missing data mechanism is nonignorable</font>. In: <em>Biometrika</em> 88.2 (2001), pp. 551-564.</cite></pref>
<div>
<a href="https://doi.org/10.1093/biomet/88.2.551" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Ibrahim, J. G., S. R. Lipsitz, and M. Chen. <font color="#428bca">Missing Covariates in Generalized Linear Models When the Missing Data Mechanism is Non-Ignorable</font>. In: <em>Journal of the Royal Statistical Society</em>. Series B (Statistical Methodology) 61.1 (1999), pp. 173-190.</cite></pref>
<div>
<a href="https://doi.org/10.1111/1467-9868.00170" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a><a href="http://www.jstor.org/stable/2680744" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Ipsen, N. B., P. Mattei, and J. Frellsen. <font color="#428bca">not-MIWAE: Deep generative modelling with missing not at random data</font>. In: <em>arXiv preprint</em> (2020).</cite></pref>
<div>
<a href="https://arxiv.org/abs/2006.12871" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Jamshidian, M., S. Jalal, and C. Jansen. <font color="#428bca">MissMech: an R package for testing homoscedasticity, multivariate normality, and missing completely at random (MCAR)</font>. In: <em>Journal of Statistical Software</em> 56.6 (2014), pp. 1-31.</cite></pref>
<div>
<a href="https://doi.org/10.18637/jss.v056.i06" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Jamshidian, M. and S. Jalal. <font color="#428bca">Tests of homoscedasticity, normality, and missing completely at random for incomplete multivariate data</font>. In: <em>Psychometrika</em> 75.4 (2010), pp. 649-674. eprint: NIHMS150003.</cite></pref>
<div>
<a href="https://doi.org/10.1007/s11336-010-9175-3" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Larose, C., D. K. Dey, and O. Harel. <font color="#428bca">The impact of missing values on different measures of uncertainty</font>. In: <em>Statistica Sinica</em> 29.2 (2019), pp. 551–566.</cite></pref>
<div>
<a href="https://doi.org/10.5705/ss.202016.0073" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Lee, K. M., R. Mitra, and S. Biedermann. <font color="#428bca">Optimal design when outcome values are not missing at random</font>. In: <em>Statistica Sinica</em> 28.4 (2018), pp. 1821–1838.</cite></pref>
<div>
<a href="https://doi.org/10.5705/ss.202016.0526" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Lee, K. J., K. Tilling, R. P. Cornish, et al. <font color="#428bca">Framework for the Treatment And Reporting of Missing data in Observational Studies: The Treatment And Reporting of Missing data in Observational Studies framework</font>. In: <em>Journal of clinical epidemiology</em> 134 (2021), pp. 79–88.</cite></pref></li>
<li><pref><cite>Little, R. J. A. <font color="#428bca">A test of missing completely at random for multivariate data with missing values</font>. In: <em>Journal of the American Statistical Association</em> 83.404 (1988), pp. 1198-1202.</cite></pref>
<div>
<a href="https://doi.org/10.2307/2290157" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Little, R. J. A. <font color="#428bca">Pattern-mixture models for multivariate incomplete data</font>. In: <em>Journal of the American Statistical Association</em> 88.421 (1993), pp. 125-134.</cite></pref>
<div>
<a href="https://doi.org/10.2307/2290705" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Little, R. J. A. <font color="#428bca">Modeling the drop-out mechanism in repeated-measures studies</font>. In: <em>Journal of the American Statistical Association</em> 90.431 (1995), pp. 1112-1121.</cite></pref>
<div>
<a href="https://doi.org/10.2307/2291350" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Miao, W. and E. J. Tchetgen Tchetgen. <font color="#428bca">Identification and inference with nonignorable missing covariate data</font>. In: <em>Statistica Sinica</em> 28.4 (2018), pp. 2049–2067.</cite></pref>
<div>
<a href="https://doi.org/10.5705/ss.202016.0322" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Molenberghs, G., B. Michiels, M. G. Kenward, et al. <font color="#428bca">Monotone missing data and pattern-mixture models</font>. In: <em>Statistica Neerlandica</em> 52.2 (1998), pp. 153-161.</cite></pref>
<div>
<a href="https://doi.org/10.1111/1467-9574.00075" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Nabi, R., R. Bhattacharya, and I. Shpitser. <font color="#428bca">Full Law Identification In Graphical Models Of Missing Data: Completeness Results</font>. In: <em>arXiv preprint arXiv:2004.04872</em> (2020).</cite></pref>
<div>
<a href="https://arxiv.org/abs/2004.04872" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Reiter, J. P. and M. Sadinle. <font color="#428bca">Itemwise conditionally independent nonresponse modelling for incomplete multivariate data</font>. In: <em>Biometrika</em> 104.1 (Jan. 2017), pp. 207-220. eprint: <a href="http://oup.prod.sis.lan/biomet/article-pdf/104/1/207/13066719/asw063.pdf" class="uri">http://oup.prod.sis.lan/biomet/article-pdf/104/1/207/13066719/asw063.pdf</a>.</cite></pref>
<div>
<a href="https://doi.org/10.1093/biomet/asw063" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a><a href="https://doi.org/10.1093/biomet/asw063" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Rioux, C., A. Lewin, O. A. Odejimi, et al. <font color="#428bca">Reflection on modern methods: planned missing data designs for epidemiological research</font>. In: <em>International Journal of Epidemiology</em> (2020).</cite></pref>
<div>
<a href="https://doi.org/10.1093/ije/dyaa042" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Robins, J. M., A. Rotnitzky, and L. P. Zhao. <font color="#428bca">Analysis of semiparametric regression models for repeated outcomes in the presence of missing data</font>. In: <em>Journal of the American Statistical Association</em> 90.429 (1995), pp. 106-121.</cite></pref>
<div>
<a href="https://doi.org/10.2307/2291134" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Rotnitzky, A., J. M. Robins, and D. O. Scharfstein. <font color="#428bca">Semiparametric regression for repeated outcomes with nonignorable nonresponse</font>. In: <em>Journal of the American Statistical Association</em> 93.444 (1998), pp. 1321-1339.</cite></pref>
<div>
<a href="https://doi.org/10.2307/2670049" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Sadinle, M. and J. P. Reiter. <font color="#428bca">Sequential Identification of Nonignorable Missing Data Mechanisms</font>. In: <em>Statistica Sinica</em> 28.4 (2018), pp. 1741–1759.</cite></pref>
<div>
<a href="https://doi.org/10.5705/ss.202016.0328" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Sadinle, M. and J. P. Reiter. <font color="#428bca">Sequentially additive nonignorable missing data modeling using auxiliary marginal information</font>. In: <em>arXiv preprint</em> (2019). arXiv: 1902.06043 [stat.ME].</cite></pref>
<div>
<a href="https://arxiv.org/pdf/1902.06043.pdf" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Santos, M. S., R. C. Pereira, A. F. Costa, et al. <font color="#428bca">Generating Synthetic Missing Data: A Review by Missing Mechanism</font>. In: <em>IEEE Access</em> 7 (2019), pp. 11651–11667.</cite></pref> <pref><cite>— <font color="#428bca">Generating Synthetic Missing Data: A Review by Missing Mechanism</font>. In: <em>IEEE Access</em> 7 (2019), pp. 11651–11667.</cite></pref>
<div>
<a href="https://doi.org/10.1109/ACCESS.2019.2891360" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Santos, M. S., R. C. Pereira, A. F. Costa, et al. <font color="#428bca">Generating Synthetic Missing Data: A Review by Missing Mechanism</font>. In: <em>IEEE Access</em> 7 (2019), pp. 11651–11667.</cite></pref>
<div>
<a href="https://doi.org/10.1109/ACCESS.2019.2891360" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Seaman, S., J. Galati, D. Jackson, et al. <font color="#428bca">What Is Meant by "Missing at Random"?</font> In: <em>Statistical Science</em> 28.2 (2013), pp. 257–268.</cite></pref> <pref><cite>— <font color="#428bca">What Is Meant by" Missing at Random"?</font> In: <em>Statistical Science</em> (2013), pp. 257–268.</cite></pref></li>
<li><pref><cite>Seaman, S., J. Galati, D. Jackson, et al. <font color="#428bca">What Is Meant by "Missing at Random"?</font> In: <em>Statistical Science</em> 28.2 (2013), pp. 257–268.</cite></pref>
<div>
<a href="https://doi.org/10.2307/43288491" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a><a href="http://www.jstor.org/stable/43288491" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Shao, J. and J. Zhang. <font color="#428bca">A transformation approach in linear mixed-effects models with informative missing responses</font>. In: <em>Biometrika</em> 102.1 (2015), pp. 107-119.</cite></pref>
<div>
<a href="https://doi.org/10.1093/biomet/asu069" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Simon, G. A. and J. S. Simonoff. <font color="#428bca">Diagnostic plots for missing data in least squares regression</font>. In: <em>Journal of the American Statistical Association</em> 81.394 (1986), pp. 501-509.</cite></pref>
<div>
<a href="https://doi.org/10.1080/01621459.1986.10478296" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Stubbendick, A. L. and J. G. Ibrahim. <font color="#428bca">Maximum Likelihood Methods for Nonignorable Missing Responses and Covariates in Random Effects Models</font>. In: <em>Biometrics</em> 59.4 (2003), pp. 1140–1150.</cite></pref>
<div>
<a href="https://doi.org/10.1111/j.0006-341X.2003.00131.x" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Stubbendick, A. L. and J. G. Ibrahim. <font color="#428bca">Likelihood-based inference with nonignorable missing responses and covariates in models for discrete longitudinal data</font>. In: <em>Statistica Sinica</em> 16.4 (2006), pp. 1143–1167.</cite></pref>
<div>
<a href="https://www.jstor.org/stable/24307781" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Tchetgen Tchetgen, E. J., L. Wang, and B. Sun. <font color="#428bca">Discrete choice models for nonmonotone nonignorable missing data: identification and inference</font>. In: <em>Statistica Sinica</em> 28.4 (2018), pp. 2069–2088.</cite></pref>
<div>
<a href="https://doi.org/10.5705/ss.202016.0325" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Templ, M., A. Alfons, and P. Filzmoser. <font color="#428bca">Exploring Incomplete data using visualization techniques</font>. In: <em>Advances in Data Analysis and Classification</em> 6.1 (2012), pp. 29-47.</cite></pref>
<div>
<a href="https://doi.org/10.1007/s11634-011-0102-y" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Thijs, H., G. Molenberghs, B. Michiels, et al. <font color="#428bca">Strategies to fit pattern-mixture models</font>. In: <em>Biostatistics</em> 3.2 (2002), pp. 245-265.</cite></pref>
<div>
<a href="https://doi.org/10.1093/biostatistics/3.2.245" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Tierney, N. J., F. A. Harden, M. J. Harden, et al. <font color="#428bca">Using decision trees to understand structure in missing data</font>. In: <em>BMJ Open</em> 5.6 (2015), p. e007450.</cite></pref>
<div>
<a href="https://doi.org/10.1136/bmjopen-2014-007450" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Vansteelandt, S., A. Rotnitzky, and J. Robins. <font color="#428bca">Estimation of regression models for the mean of repeated outcomes under nonignorable nonmonotone nonresponse</font>. In: <em>Biometrika</em> 94.4 (2007), pp. 841–860.</cite></pref>
<div>
<a href="https://doi.org/10.1093/biomet/asm070" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Verbeke, G., G. Molenberghs, H. Thijs, et al. <font color="#428bca">Sensitivity analysis for nonrandom dropout: a local influence approach</font>. In: <em>Biometrics</em> 57.1 (2001), pp. 7-14.</cite></pref>
<div>
<a href="https://doi.org/10.1111/j.0006-341X.2001.00007.x" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>White, I. R., J. Carpenter, and N. J. Horton. <font color="#428bca">A mean score method for sensitivity analysis to departures from the missing at random assumption in randomised trials</font>. In: <em>Statistica Sinica</em> 28.4 (2018), pp. 1985–2003.</cite></pref>
<div>
<a href="https://doi.org/10.5705/ss.202016.0308" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Wu, M. C. and R. J. Carroll. <font color="#428bca">Estimation and comparison of changes in the presence of informative right censoring by modeling the censoring process</font>. In: <em>Biometrics</em> 44.1 (1988), pp. 175-188.</cite></pref>
<div>
<a href="https://doi.org/10.2307/2531905" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Zhao, J. and Y. Ma. <font color="#428bca">A versatile estimation procedure without estimating the nonignorable missingness mechanism</font>. In: <em>Journal of the American Statistical Association</em> (2021), pp. 1–15.</cite></pref>
<div>
<a href="https://doi.org/10.1080/01621459.2021.1893176" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Zhou, Y., R. J. A. Little, and J. D. Kalbfleisch. <font color="#428bca">Block-conditional missing at random models for missing data</font>. In: <em>Statistical Science</em> 25.4 (2010), pp. 517–532.</cite></pref>
<div>
<a href="https://doi.org/10.1214/10-STS344" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
</ul>
</div>
</div>
<!-- Conference papers -->
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#mnar_conf">
Conference papers (3)
</button>
<div id="mnar_conf" class="collapse">
<ul>
<li><pref><cite>Gill, R. D., M. J. Van Der Laan, and J. M. Robins. <font color="#428bca">Coarsening at random: Characterizations, conjectures, counter-examples</font>. In: <em>Proceedings of the First Seattle Symposium in Biostatistics</em>. Springer. 1997, pp. 255–294.</cite></pref>
<div>
<a href="https://doi.org/10.1007/978-1-4684-6316-3_14" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a>
</div></li>
<li><pref><cite>Mohan, K., F. Thoemmes, and J. Pearl. <font color="#428bca">Estimation with Incomplete Data: The Linear Case</font>. In: <em>Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18</em>. International Joint Conferences on Artificial Intelligence Organization, Jul. 2018, pp. 5082–5088.</cite></pref>
<div>
<a href="https://doi.org/10.24963/ijcai.2018/705" target="blank" role="button" class="btn btn-outline-secondary btn-sm" >DOI</a><a href="https://doi.org/10.24963/ijcai.2018/705" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Sportisse, A., C. Boyer, and J. Josse. <font color="#428bca">Estimation with informative missing data in the low-rank model with random effects</font>. In: <em>Advances in Neural Information Processing Systems, 33</em>. (Dec. 2020). Ed. by -. IEEE, 2020. eprint: 1906.02493v3.</cite></pref>
<div>
<a href="https://arxiv.org/abs/1906.02493" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
</ul>
</div>
</div>
<!-- Reports, theses, etc. -->
<div class="container">
<button type="button" class="btn btn-sm" data-toggle="collapse" data-target="#mnar_misc">
Reports, theses, etc. (2)
</button>
<div id="mnar_misc" class="collapse">
<ul>
<li><pref><cite>Mohan, K. and J. Pearl. <em><font color="#428bca">Graphical Models for Processing Missing Data</font></em>. Tech. rep. R-473-L. Forthcoming, Journal of American Statistical Association (JASA). CA: Department of Computer Science, University of California, Los Angeles, 2019.</cite></pref>
<div>
<a href="http://ftp.cs.ucla.edu/pub/stat\_ser/r473-L.pdf" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
<li><pref><cite>Tierney, N. and D. Cook. <em><font color="#428bca">Expanding tidy data principles to facilitate missing data exploration, visualization and assessment of imputations</font></em>. Monash Econometrics and Business Statistics Working Papers 14/18. Monash University, Department of Econometrics and Business Statistics, 2018.</cite></pref>
<div>
<a href="https://ideas.repec.org/p/msh/ebswps/2018-14.html" target="_blank" role="button" class="btn btn-outline-secondary btn-sm" >URL</a>
</div></li>
</ul>
</div>
</div>
<br>
</div>
</div>
</div>
</div>
<div class="accordion-option">
<a href="javascript:void(0)" class="toggle-accordion active" accordion-id="#accordion"></a>
</div>
</div>
<style type="text/css">
.panel-group {
	padding: 0;
	margin-left: -15px;
}
.panel-default>.panel-heading {
  color: #333;
  background-color: #fff;
  border-color: #e4e5e7;
  padding: 0;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}
.panel-default>.panel-heading a {
  display: block;
  padding: 10px 15px;
}
.panel-collapse>.panel-body {
  padding: 20px 30px;
  margin-left: 30px;
}
.panel-default>.panel-heading a:after {
  content: "";
  position: relative;
  top: 1px;
  display: inline-block;
  font-style: normal;
  font-weight: 400;
  line-height: 1;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  float: right;
  transition: transform .25s linear;
  -webkit-transition: -webkit-transform .25s linear;
}
.panel-default>.panel-heading a[aria-expanded="true"] {
  background-color: #eee;
}
.panel-default>.panel-heading a[aria-expanded="true"]:after {
  content: "\2212";
  -webkit-transform: rotate(180deg);
  transform: rotate(180deg);
}
.panel-default>.panel-heading a[aria-expanded="false"]:after {
  content: "\002b";
  -webkit-transform: rotate(90deg);
  transform: rotate(90deg);
}
.accordion-option {
  width: 100%;
  float: left;
  clear: both;
  margin: 0px 0;
}
.accordion-option .title {
  font-size: 20px;
  font-weight: bold;
  float: left;
  padding: 0;
  margin: 0;
}
.accordion-option .toggle-accordion {
  float: right;
  font-size: 16px;
  color: #6a6c6f;
}
.accordion-option .toggle-accordion:before {
  content: "Collapse All";
}
.accordion-option .toggle-accordion.active:before {
  content: "Collapse All";
}
</style>
<script>
$(document).ready(function() {
  $(".toggle-accordion").on("click", function() {
    var accordionId = $(this).attr("accordion-id"),
      numPanelOpen = $(accordionId + ' .collapse.in').length;
    
    $(this).toggleClass("active");
    if (numPanelOpen == 0) {
      openAllPanels(accordionId);
    } else {
      closeAllPanels(accordionId);
    }
  });
  openAllPanels = function(aId) {
    console.log("setAllPanelOpen");
    $(aId + ' .panel-collapse:not(".in")').collapse("show");
  };
  closeAllPanels = function(aId) {
    console.log("setAllPanelclose");
    $(aId + ' .panel-collapse.in').collapse("hide");
  };
});
</script>
